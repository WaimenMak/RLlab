{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231903 entries, 0 to 260086\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   embedded_state   231903 non-null  object\n",
      " 1   embedded_action  231903 non-null  object\n",
      " 2   reward           231903 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 310.1 MB\n",
      "None\n",
      "16.62066960334778\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "from processing import train_data, embed\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class itemEnv():\n",
    "    def __init__(self, alpha = 0.4, sigma = 0.9): # albha parameter in consine similarity\n",
    "        self.observation_space = train_data\n",
    "        self.item_info = embed\n",
    "        self.current_state = None\n",
    "        self.init_state = self.reset()\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.embedding_dim = feature_num\n",
    "        self.reward_val = 1\n",
    "#         self.rewards, self.avg_state, self.avg_action, self.nx_size = self.avg_group()\n",
    "    \n",
    "    def reset(self):\n",
    "        init_state = self.observation_space['embedded_state'].sample(1).values[0]\n",
    "#         init_state = self.observation_space.loc[7,'embedded_state']\n",
    "        self.current_state = init_state\n",
    "        return init_state\n",
    "\n",
    "#     def feedback(self, pair):  # pair: state-action pair\n",
    "#         prob = list()\n",
    "#         denominator = 0.\n",
    "#         max_prob = 0.\n",
    "#         result = 0.\n",
    "#         feed_back = \"\"\n",
    "#         for r, s, a in zip(self.rewards, self.avg_state, self.avg_action):\n",
    "#             numerator = np.dot(pair[0], s.T) / np.linalg.norm(pair[0], 2) + np.dot(pair[1], a.T) / np.linalg.norm(pair[1], 2)\n",
    "#             denominator += numerator\n",
    "#             prob.append(numerator)\n",
    "#             if numerator > max_prob:\n",
    "#                 max_prob = numerator\n",
    "#                 feed_back = r\n",
    "#         prob = prob/denominator\n",
    "# #         for p, r in zip(prob, self.rewards):\n",
    "# #             for k in range(1):\n",
    "# #                 result += p * self.reward_val * np.power(self.sigma, k) * int(r.split(',')[k])\n",
    "        \n",
    "#         result = self.reward_val * int(feed_back.split(',')[0])\n",
    "#         print(prob)\n",
    "#         return feed_back, result\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(group, pair):\n",
    "        max_prob = 0.\n",
    "        # nx_size = len(group[1])\n",
    "        s_i = np.array(group[1]['embedded_state'].values.tolist()) # 2 dim array:0-->sample, 1-->feature_num (N,(7*12))\n",
    "        s_t = pair[0]                                              # 1 dim array:0-->sample, (7*12,)\n",
    "        a_i = np.array(group[1]['embedded_action'].values.tolist()) #(N,9,7)\n",
    "        a_t = pair[1]                                               #(7,)\n",
    "        norm_si = np.linalg.norm(s_i, 2, axis = 1)                 # 1 dim (N,)\n",
    "        norm_st = np.linalg.norm(s_t, 2, axis = 0)                 # 1 dim (1,)\n",
    "        norm_ai = np.linalg.norm(a_i, 2, axis = 2)                 # 2 dim (N, 9)\n",
    "        norm_at = np.linalg.norm(a_t, 2, axis = 0)                 # 1 dim (1,)\n",
    "        \n",
    "        #first term: (N,1), second term: (N,9)\n",
    "        cos = env.alpha * (np.dot(s_i, s_t)/(norm_si * norm_st + 1e-10))[:,np.newaxis] + (1 - env.alpha) * np.dot(a_i, a_t)/(norm_ai * norm_at + 1e-10)  #1 dim\n",
    "        # cos = cos / np.sum(cos)\n",
    "        #cos: (N,9)\n",
    "        max_id = np.argmax(cos)\n",
    "        id_x = int(max_id / cos.shape[1])\n",
    "        id_y = max_id % cos.shape[1]\n",
    "        # print('i am here!')\n",
    "        return (np.max(cos), id_x, id_y)\n",
    "    \n",
    "    def feedback_2(self, pair):\n",
    "        data = train_data.groupby(['reward'])\n",
    "#         denominator = 0.\n",
    "        reward_ind = -1\n",
    "        max_prob = 0.\n",
    "        result = 0.\n",
    "        feed_back = \"\"\n",
    "        fnc = partial(self.cos_sim, pair = pair)\n",
    "        prob = map(fnc, data)\n",
    "        for tup, group in zip(prob, data):\n",
    "            if tup[0] > max_prob:\n",
    "                max_prob = tup[0]\n",
    "                reward_ind = tup[2]  #id_y\n",
    "                feed_back = group[0]          #index: reward str\n",
    "#                 feed_back = group[1].iloc[tup[1], 2]\n",
    "#                 idx = tup[1]\n",
    "\n",
    "        r = 1 * int(feed_back.split(',')[reward_ind])  #reward_val\n",
    "        result =  r if r > 0 else -1\n",
    "\n",
    "        return feed_back, result\n",
    "            \n",
    "\n",
    "    def step(self, action):   #action: 1 dim array\n",
    "        feed_back, result = self.feedback_2((self.current_state, action))\n",
    "#         for i,r in enumerate([feed_back.split(',')[0]]): #0---> one action\n",
    "        if result == 1:     #reward_val\n",
    "            tmp = np.append(self.current_state, action)\n",
    "            tmp = tmp[self.embedding_dim:]\n",
    "            self.current_state = tmp  # state: 1 dim array\n",
    "#         else:\n",
    "#             self.current_state = self.observation_space['embedded_state'].sample(1).values[0]\n",
    "\n",
    "        return result, self.current_state, feed_back\n",
    "        \n",
    "#     def avg_group(self):\n",
    "#         nx_size = list()\n",
    "#         avg_state = list()\n",
    "#         avg_action = list()\n",
    "#         rewards = list()\n",
    "#         for reward, group in self.observation_space.groupby(['reward']):\n",
    "#             nx_size.append(group.shape[0])\n",
    "# #             state = np.mean(data['embedded_action'], axis = 0)\n",
    "#             norm_s = np.linalg.norm(np.array(group['embedded_state'].values.tolist()), 2, axis = 1)\n",
    "#             norm_s = np.where(norm_s == 0, 0.001, norm_s)\n",
    "#             state = np.sum(group['embedded_state'] / norm_s) / group.shape[0]\n",
    "#             norm_a = np.linalg.norm(np.array(group['embedded_action'].values.tolist()), 2, axis = 1)\n",
    "#             norm_a = np.where(norm_a == 0, 0.001, norm_a)\n",
    "#             action = np.sum(group['embedded_action'] / norm_a) / group.shape[0]\n",
    "#             avg_state.append(state)\n",
    "#             avg_action.append(action)\n",
    "#             rewards.append(reward)\n",
    "            \n",
    "#         return rewards, avg_state, avg_action, nx_size\n",
    "def gen_action(item_id):\n",
    "    return embed[item_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = itemEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.07092139,  2.46203633, -0.72387844, -0.12012139,\n",
       "         1.63520295, -0.73754373, -2.24228081,  0.97162306,  0.92553209,\n",
       "        -0.72387844, -0.10724431,  1.96710076, -0.89260205, -2.24228081,\n",
       "         0.07092139,  2.8461624 , -0.72387844, -0.12255155,  1.60967234,\n",
       "        -0.98937603, -2.24228081,  0.97162306,  2.8461624 , -0.72387844,\n",
       "        -0.12364443,  0.65774564, -0.84696432, -0.75910548, -0.82978028,\n",
       "        -0.61097216, -0.72387844, -0.12394348,  0.51550372, -0.92394362,\n",
       "        -0.75910548,  1.87232473, -0.2268461 , -0.72387844, -0.127393  ,\n",
       "         1.15741598, -0.66166413, -0.75910548,  0.07092139,  1.30965815,\n",
       "        -0.72387844, -0.11604297,  0.35502565,  2.16787524,  0.72406985,\n",
       "         0.07092139,  1.69378421, -0.72387844, -0.10893131,  1.20847718,\n",
       "         0.15651593,  0.72406985,  0.97162306,  0.92553209, -0.72387844,\n",
       "        -0.10724431,  1.96710076, -0.89260205, -2.24228081,  1.87232473,\n",
       "        -0.61097216, -0.72387844, -0.11864104,  2.04369257, -0.81397318,\n",
       "        -2.24228081,  0.97162306, -0.61097216, -0.72387844, -0.12460418,\n",
       "         1.43825259, -0.80352599, -2.24228081,  0.07092139,  1.69378421,\n",
       "        -0.72387844, -0.11545878,  1.79568101, -0.16789686, -0.75910548]), '0,0,0,0,0,0,0,0,0', 0.609839677810669)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "start = time.time()\n",
    "# action = train_data['embedded_action'].sample(1).values[0][5]\n",
    "# action = train_data.loc[0,'embedded_action'][0]\n",
    "action = gen_action(str(randint(1, 381)))\n",
    "r,s,f = env.step(action)\n",
    "end = time.time()\n",
    "r, s, f,end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([ 0.07092139,  1.69378421, -0.72387844, -0.10893131,  1.20847718,\n",
       "         0.15651593,  0.72406985,  0.07092139,  0.54140602, -0.72387844,\n",
       "        -0.11866091,  1.73732535, -0.82606993, -2.24228081,  0.07092139,\n",
       "        -0.61097216, -0.72387844, -0.12115068,  1.52213885, -1.04436125,\n",
       "        -2.24228081,  1.87232473, -0.61097216, -0.72387844, -0.12357091,\n",
       "         1.2230661 , -0.93494067, -2.24228081,  0.07092139, -0.61097216,\n",
       "        -0.72387844, -0.12258235,  1.53308054, -1.02236716, -0.75910548,\n",
       "         1.87232473,  0.15727996, -0.72387844, -0.11585022,  1.29601067,\n",
       "        -0.53959694, -0.75910548,  0.97162306,  0.92553209, -0.72387844,\n",
       "        -0.12226144,  1.55861114, -0.69410541, -0.75910548,  1.87232473,\n",
       "        -0.61097216, -0.72387844, -0.11926497,  0.36961457, -0.77438383,\n",
       "         0.72406985,  1.87232473,  0.92553209, -0.72387844, -0.11692224,\n",
       "         0.96776008, -0.8805053 ,  0.72406985,  1.87232473, -0.61097216,\n",
       "        -0.72387844, -0.11296006,  1.54402223, -0.8299189 , -2.24228081,\n",
       "         1.87232473, -0.61097216, -0.72387844, -0.12357091,  1.2230661 ,\n",
       "        -0.93494067, -2.24228081,  0.07092139,  0.54140602, -0.72387844,\n",
       "        -0.11866091,  1.73732535, -0.82606993, -2.24228081,  1.87232473,\n",
       "         0.15727996, -0.72387844, -0.11585022,  1.29601067, -0.53959694,\n",
       "        -0.75910548,  0.07092139, -0.61097216, -0.72387844, -0.12258235,\n",
       "         1.53308054, -1.02236716, -0.75910548,  0.07092139,  2.8461624 ,\n",
       "        -0.72387844, -0.12359178,  0.93858225, -1.00037308, -0.75910548,\n",
       "         1.87232473, -0.61097216, -0.72387844, -0.12207764,  0.53373986,\n",
       "        -0.5044064 ,  0.72406985,  0.97162306,  2.07791027, -0.72387844,\n",
       "        -0.12436871,  0.20548927,  0.54306202,  0.72406985, -0.82978028,\n",
       "         1.69378421, -0.72387844, -0.10210977,  0.60668444,  0.92300988,\n",
       "         0.72406985,  0.97162306, -0.2268461 , -0.72387844, -0.12719231,\n",
       "         1.08082417, -0.36804306, -0.75910548,  0.97162306,  0.15727996,\n",
       "        -0.72387844, -0.11658345,  2.09110654, -0.07497184, -2.24228081]), '1,1,1,1,1,1,1,1,0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = gen_action(str(randint(1, 381)))\n",
    "r,s,f= env.step(action)\n",
    "r, s, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward\n",
       "0,0,0,0,0,0,0,0,0    21658\n",
       "0,0,1,0,0,0,0,0,0     5885\n",
       "0,1,0,0,0,0,0,0,0     6027\n",
       "0,1,0,1,0,0,1,0,0        1\n",
       "0,1,1,0,0,0,0,0,0     2940\n",
       "1,0,0,0,0,0,0,0,0     7280\n",
       "1,0,1,0,0,0,0,0,0     3228\n",
       "1,1,0,0,0,0,0,0,0     4519\n",
       "1,1,0,0,1,0,0,0,1        1\n",
       "1,1,1,0,0,0,0,0,0     5187\n",
       "1,1,1,0,0,1,0,0,0     4040\n",
       "1,1,1,0,1,0,0,0,0     3852\n",
       "1,1,1,0,1,1,0,0,0     3051\n",
       "1,1,1,1,0,0,0,0,0     4639\n",
       "1,1,1,1,0,1,0,0,0     3212\n",
       "1,1,1,1,1,0,0,0,0     4639\n",
       "1,1,1,1,1,1,0,0,0     3385\n",
       "1,1,1,1,1,1,0,0,1     4730\n",
       "1,1,1,1,1,1,0,1,0     7954\n",
       "1,1,1,1,1,1,0,1,1     6974\n",
       "1,1,1,1,1,1,1,0,0    10164\n",
       "1,1,1,1,1,1,1,0,1     8643\n",
       "1,1,1,1,1,1,1,1,0    33067\n",
       "1,1,1,1,1,1,1,1,1    26984\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('reward').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as act\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(): #single gpu\n",
    "    i = 0\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, inpt_sz, hidden_sz, hidden_l1, hidden_l2, oupt_sz): #embedding dim, , , 381\n",
    "        super().__init__()\n",
    "        #input_gate\n",
    "        device = try_gpu()\n",
    "        init_weights = partial(self.init_, device = device)\n",
    "#         self.init_state = None\n",
    "        self.reward = []\n",
    "        self.inpt_sz = inpt_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.hidden_l1 = hidden_l1\n",
    "        self.hidden_l2 = hidden_l2\n",
    "        self.oupt_sz = oupt_sz\n",
    "        self.W_xi, self.W_hi, self.bias_i = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #forget_gate\n",
    "        self.W_xf, self.W_hf, self.bias_f = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #output_gate\n",
    "        self.W_xo, self.W_ho, self.bias_o = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #candidate memory cell\n",
    "        self.W_xc, self.W_hc, self.bias_c = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #output_layer\n",
    "        self.W_o1, self.W_o2, self.bias_o1, self.bias_o2 = (init_weights((hidden_sz, hidden_l1)), init_weights(\n",
    "                                            (hidden_l1, hidden_l2)), init_weights((1, hidden_l1)), \n",
    "                                            init_weights((1, hidden_l2))\n",
    "                                            ) \n",
    "        self.W_o3, self.bias_o3 = (init_weights((hidden_l2, oupt_sz)), \n",
    "                                            init_weights((1, oupt_sz)))\n",
    "    @staticmethod\n",
    "    def init_(shape, device):\n",
    "#         param = torch.tensor(shape)\n",
    "        def xvaier(param):\n",
    "            return nn.init.xavier_uniform_(param)\n",
    "#         param = xvaier(torch.rand(shape, device = device))\n",
    "        param = torch.randn(shape, device = device) * 0.01\n",
    "        return nn.Parameter(param)\n",
    "    \n",
    "    def forward(self, X, init_state = None):  #X: batch size, seq size, input size\n",
    "        batch_size, seq_size, _ = X.shape\n",
    "        hidden_sz = self.hidden_sz\n",
    "#         oupts = []\n",
    "        if init_state == None:         #H, C actually are constant, not trainable\n",
    "            H, C = (torch.zeros(batch_size, hidden_sz, device = X.device),\n",
    "                    torch.zeros(batch_size, hidden_sz, device = X.device)\n",
    "                   )\n",
    "        else:\n",
    "            H, C = init_state    # in some circumstance\n",
    "#         softmax = act.softmax(dim = 1)\n",
    "        for seq in range(seq_size):\n",
    "            x_t = X[:,seq,:]\n",
    "            I = torch.sigmoid(x_t @ self.W_xi + H @ self.W_hi + self.bias_i)\n",
    "            F = torch.sigmoid(x_t @ self.W_xf + H @ self.W_hf + self.bias_f)\n",
    "            O = torch.sigmoid(x_t @ self.W_xo + H @ self.W_ho + self.bias_o)\n",
    "            C_tilda = torch.tanh(x_t @ self.W_xc + H @ self.W_hc + self.bias_c)\n",
    "            \n",
    "            C = F * C + I * C_tilda\n",
    "            H = torch.tanh(C) + O\n",
    "            hidden_layer1 = torch.sigmoid(H @ self.W_o1 + self.bias_o1)\n",
    "            hidden_layer2 = torch.relu(hidden_layer1 @ self.W_o2 + self.bias_o2)\n",
    "            output = act.softmax(hidden_layer2 @ self.W_o3 + self.bias_o3, dim = 1)\n",
    "#             output = torch.sigmoid(hidden_layer2 @ self.W_o3 + self.bias_o3)\n",
    "#             output = torch.relu(hidden_layer1 @ self.W_o2 + self.bias_o2)\n",
    "#             oupts.append(output)\n",
    "            \n",
    "#         self.init_state = (H, C)\n",
    "#         return torch.cat(oupts, dim = 0)\n",
    "        return output, (H, C)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = LSTM(feature_num, 64, 128, 256, 382)\n",
    "# actor.load_state_dict(torch.load('modelpara.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339:-1 186:-1 36:1 224:1 205:-1 total reward:-0.8460999999999997, loss:-5.061864376068115\n",
      "92:1 227:-1 202:-1 219:-1 79:1 total reward:-2.9539000000000004, loss:-17.56693458557129\n",
      "24:1 203:-1 36:1 177:-1 147:1 total reward:2.4661, loss:14.642876625061035\n",
      "246:-1 56:1 308:1 238:1 170:-1 total reward:2.9539000000000004, loss:17.615970611572266\n",
      "220:-1 231:-1 13:-1 273:-1 144:-1 total reward:-13.1441, loss:-78.23262023925781\n",
      "59:-1 130:-1 47:-1 325:-1 204:-1 total reward:-13.1441, loss:-78.15579223632812\n",
      "144:-1 151:-1 359:-1 168:-1 63:-1 total reward:-13.1441, loss:-77.99214172363281\n",
      "222:-1 153:-1 205:-1 297:1 279:-1 total reward:-6.266100000000001, loss:-37.27794647216797\n",
      "182:-1 24:-1 222:-1 6:-1 139:1 total reward:-4.9539, loss:-29.53072166442871\n",
      "166:-1 352:-1 40:1 354:-1 149:-1 total reward:-7.7241, loss:-45.93189239501953\n",
      "270:-1 373:-1 24:1 97:-1 374:-1 total reward:-7.7241, loss:-45.97041702270508\n",
      "61:-1 43:-1 56:-1 40:-1 9:1 total reward:-4.9539, loss:-29.356725692749023\n",
      "311:-1 195:-1 20:1 113:1 265:1 total reward:7.344100000000001, loss:43.63983154296875\n",
      "310:-1 235:-1 278:-1 97:1 48:-1 total reward:-6.266100000000001, loss:-37.149070739746094\n",
      "222:-1 288:-1 321:-1 46:-1 265:1 total reward:-4.9539, loss:-29.680950164794922\n",
      "242:-1 374:-1 306:-1 220:1 316:1 total reward:1.9241, loss:11.558265686035156\n",
      "117:-1 93:-1 345:-1 348:-1 298:-1 total reward:-13.1441, loss:-78.15621185302734\n",
      "41:-1 141:1 334:-1 1:1 209:1 total reward:5.7241, loss:33.98848342895508\n",
      "46:-1 5:-1 183:-1 180:-1 20:1 total reward:-4.9539, loss:-29.49708366394043\n",
      "45:-1 40:-1 2:1 78:1 242:1 total reward:7.344100000000001, loss:43.7425651550293\n",
      "175:-1 200:1 257:-1 132:1 4:1 total reward:5.7241, loss:34.064613342285156\n",
      "271:-1 266:-1 248:1 83:-1 154:-1 total reward:-7.7241, loss:-45.86861801147461\n",
      "124:-1 122:-1 194:-1 166:-1 358:-1 total reward:-13.1441, loss:-78.23751831054688\n",
      "220:-1 223:-1 124:-1 116:-1 78:-1 total reward:-13.1441, loss:-78.33612060546875\n",
      "25:1 184:-1 228:-1 53:1 211:-1 total reward:-4.2661, loss:-25.357908248901367\n",
      "136:1 50:-1 271:-1 99:1 178:-1 total reward:-4.2661, loss:-25.392114639282227\n",
      "296:-1 82:1 253:1 52:1 190:-1 total reward:2.9539000000000004, loss:17.566741943359375\n",
      "71:1 17:1 376:-1 173:1 267:-1 total reward:-0.46609999999999996, loss:-2.846142292022705\n",
      "326:-1 302:-1 249:-1 327:1 176:-1 total reward:-6.266100000000001, loss:-37.17702865600586\n",
      "318:1 77:-1 292:-1 299:-1 305:-1 total reward:-11.1441, loss:-66.19202423095703\n",
      "58:-1 27:1 37:1 230:1 377:1 total reward:11.1441, loss:66.07182312011719\n",
      "348:-1 191:-1 106:1 327:-1 19:-1 total reward:-7.7241, loss:-45.95574951171875\n",
      "266:-1 22:1 243:1 291:-1 186:1 total reward:4.2661, loss:25.383760452270508\n",
      "127:1 23:1 301:-1 80:1 38:1 total reward:7.7241, loss:45.83154296875\n",
      "121:1 225:1 199:1 119:1 213:-1 total reward:4.9539, loss:29.432954788208008\n",
      "187:1 285:-1 282:-1 345:-1 204:-1 total reward:-11.1441, loss:-66.22915649414062\n",
      "122:1 207:-1 49:1 12:1 34:1 total reward:9.344100000000001, loss:55.608489990234375\n",
      "287:-1 11:1 55:-1 21:1 299:-1 total reward:-2.4661, loss:-14.644899368286133\n",
      "294:1 365:1 42:-1 10:1 80:1 total reward:7.7241, loss:45.85211181640625\n",
      "371:-1 147:1 237:-1 282:1 106:1 total reward:5.7241, loss:34.13808059692383\n",
      "201:-1 141:-1 112:1 11:-1 359:-1 total reward:-7.7241, loss:-45.87557601928711\n",
      "2:-1 257:-1 153:-1 64:1 48:-1 total reward:-6.266100000000001, loss:-37.0815544128418\n",
      "30:-1 213:1 3:-1 151:-1 120:1 total reward:-1.1539000000000006, loss:-6.9017767906188965\n",
      "191:1 344:-1 217:-1 220:-1 179:-1 total reward:-11.1441, loss:-66.3780746459961\n",
      "99:1 41:1 101:-1 61:-1 0:1 total reward:0.8460999999999997, loss:5.038005828857422\n",
      "99:1 222:-1 184:1 205:-1 210:1 total reward:2.4661, loss:14.671635627746582\n",
      "61:1 276:-1 205:-1 61:1 39:1 total reward:3.9241, loss:23.438806533813477\n",
      "55:-1 147:1 298:-1 265:-1 139:1 total reward:-1.1539000000000006, loss:-6.937135696411133\n",
      "42:-1 149:-1 79:1 127:1 47:-1 total reward:-0.8460999999999997, loss:-5.076953887939453\n",
      "1:1 14:1 371:-1 226:-1 272:-1 total reward:-7.344100000000001, loss:-43.52206039428711\n",
      "330:-1 133:1 327:-1 254:-1 212:1 total reward:-1.1539000000000006, loss:-6.900155544281006\n",
      "219:1 353:-1 76:1 93:1 275:-1 total reward:1.1539000000000006, loss:6.892476558685303\n",
      "102:1 100:1 67:-1 353:-1 142:1 total reward:0.8460999999999997, loss:5.004548072814941\n",
      "121:1 91:1 19:1 111:1 167:-1 total reward:4.9539, loss:29.29008674621582\n",
      "277:-1 184:1 30:1 82:1 50:1 total reward:11.1441, loss:66.40062713623047\n",
      "153:-1 36:1 320:-1 155:-1 81:1 total reward:-1.1539000000000006, loss:-6.8892340660095215\n",
      "85:1 57:-1 12:1 188:-1 194:1 total reward:2.4661, loss:14.657997131347656\n",
      "282:-1 239:-1 111:1 102:1 257:-1 total reward:-0.8460999999999997, loss:-5.063350200653076\n",
      "95:1 235:1 376:-1 149:-1 379:-1 total reward:-7.344100000000001, loss:-43.710548400878906\n",
      "272:-1 292:-1 341:-1 209:1 281:-1 total reward:-6.266100000000001, loss:-37.426475524902344\n",
      "171:-1 259:1 261:-1 278:-1 262:-1 total reward:-9.344100000000001, loss:-55.52606201171875\n",
      "101:1 305:-1 209:-1 47:-1 135:1 total reward:-2.9539000000000004, loss:-17.57571029663086\n",
      "107:1 181:1 374:-1 117:1 58:-1 total reward:-0.46609999999999996, loss:-2.7123074531555176\n",
      "182:-1 263:-1 241:-1 298:-1 338:-1 total reward:-13.1441, loss:-78.42764282226562\n",
      "22:-1 333:-1 246:-1 314:-1 143:1 total reward:-4.9539, loss:-29.24030876159668\n",
      "66:1 297:1 39:-1 231:-1 29:1 total reward:0.8460999999999997, loss:5.023116588592529\n",
      "357:1 90:1 37:1 332:-1 4:1 total reward:6.266100000000001, loss:37.20252227783203\n",
      "130:-1 61:1 125:1 335:1 255:-1 total reward:2.9539000000000004, loss:17.569992065429688\n",
      "317:1 311:1 202:-1 187:-1 182:-1 total reward:-7.344100000000001, loss:-43.91619873046875\n",
      "17:-1 182:-1 229:-1 102:-1 138:-1 total reward:-13.1441, loss:-78.67381286621094\n",
      "376:-1 94:-1 146:1 307:1 92:1 total reward:7.344100000000001, loss:43.65485382080078\n",
      "362:-1 230:1 256:-1 87:-1 337:-1 total reward:-9.344100000000001, loss:-55.35797882080078\n",
      "274:-1 364:-1 205:-1 199:-1 176:-1 total reward:-13.1441, loss:-78.12972259521484\n",
      "174:-1 64:-1 381:1 99:1 176:-1 total reward:-0.8460999999999997, loss:-5.025115489959717\n",
      "288:-1 331:-1 350:-1 345:-1 27:1 total reward:-4.9539, loss:-29.680316925048828\n",
      "316:1 46:-1 80:1 256:-1 333:-1 total reward:-5.7241, loss:-34.188682556152344\n",
      "38:-1 342:-1 47:-1 50:-1 93:1 total reward:-4.9539, loss:-29.21186065673828\n",
      "74:1 289:-1 69:1 287:1 153:-1 total reward:1.1539000000000006, loss:6.784049034118652\n",
      "228:1 31:1 347:1 347:-1 269:-1 total reward:-1.9241, loss:-11.491523742675781\n",
      "363:-1 119:1 337:-1 353:-1 328:-1 total reward:-9.344100000000001, loss:-55.48359680175781\n",
      "173:-1 157:-1 267:-1 7:1 76:1 total reward:1.9241, loss:11.422378540039062\n",
      "68:1 19:1 306:1 33:1 166:-1 total reward:4.9539, loss:29.28610610961914\n",
      "257:-1 208:-1 56:1 262:-1 136:1 total reward:0.46609999999999996, loss:2.7359423637390137\n",
      "94:1 183:1 192:1 339:-1 342:-1 total reward:-1.9241, loss:-11.570731163024902\n",
      "295:-1 212:-1 260:-1 68:1 127:1 total reward:1.9241, loss:11.257637977600098\n",
      "89:1 220:1 325:-1 297:1 350:1 total reward:7.7241, loss:46.08452606201172\n",
      "375:-1 287:1 115:1 160:-1 7:1 total reward:4.2661, loss:25.231685638427734\n",
      "39:1 361:-1 112:1 56:1 233:1 total reward:9.344100000000001, loss:55.4185791015625\n",
      "61:-1 299:1 283:-1 58:-1 142:-1 total reward:-9.344100000000001, loss:-55.240760803222656\n",
      "180:-1 279:-1 166:-1 14:-1 155:-1 total reward:-13.1441, loss:-78.47644805908203\n",
      "297:-1 184:-1 363:-1 243:-1 101:1 total reward:-4.9539, loss:-29.071313858032227\n",
      "371:-1 359:-1 250:1 82:-1 339:-1 total reward:-7.7241, loss:-46.002830505371094\n",
      "139:-1 320:1 225:-1 159:-1 186:-1 total reward:-9.344100000000001, loss:-55.338016510009766\n",
      "133:1 212:-1 131:-1 316:-1 218:-1 total reward:-11.1441, loss:-66.04942321777344\n",
      "164:-1 42:-1 57:-1 89:-1 48:-1 total reward:-13.1441, loss:-78.01469421386719\n",
      "259:-1 236:-1 322:-1 268:-1 295:-1 total reward:-13.1441, loss:-78.10553741455078\n",
      "214:-1 93:1 330:-1 324:-1 158:-1 total reward:-9.344100000000001, loss:-55.47417449951172\n",
      "121:1 94:-1 346:-1 173:-1 141:-1 total reward:-11.1441, loss:-65.55500793457031\n",
      "97:1 356:-1 198:-1 292:1 167:-1 total reward:-4.2661, loss:-25.339841842651367\n",
      "363:-1 303:-1 87:1 196:1 1:1 total reward:7.344100000000001, loss:43.625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "569.6392147541046"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "start = time.time()\n",
    "device = try_gpu()\n",
    "# loss_fn = nn.NLLLoss(reduction='sum')\n",
    "state_num = 20\n",
    "max_episodes = 50\n",
    "max_episodes_len = 5\n",
    "gamma = 0.9\n",
    "optimizer = torch.optim.SGD(actor.parameters(), lr = 0.01)\n",
    "for episode in range(max_episodes):\n",
    "    env.reset()\n",
    "    state = env.current_state\n",
    "    for j in range(2):\n",
    "    #     env.current_state = train_data.loc[7,'embedded_state']\n",
    "        env.current_state = state\n",
    "#         G_t = 0.\n",
    "    #     bat_x = []\n",
    "    #     target = []\n",
    "        policy_loss_l = []\n",
    "        reward_l = []\n",
    "        optimizer.zero_grad()\n",
    "        for i in range(max_episodes_len):\n",
    "            n_state = np.reshape(env.current_state, [-1, state_num, feature_num])\n",
    "#             n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "            n_state = torch.from_numpy(n_state).to(device).to(torch.float32)\n",
    "#             bat_x.append(n_state)\n",
    "            prob, _= actor(n_state)\n",
    "\n",
    "#             prob = act.softmax(prob, dim = 1)\n",
    "            m = Categorical(prob)\n",
    "#             action = m.sample() if np.random.random() > 0.2 else torch.randint(0, 381, [1]).cuda()  #0~380 [low, high)\n",
    "            action = m.sample() if np.random.random() > 0.2 else torch.randint(0, 382, [1]).to(device)  #0~380 [low, high)\n",
    "    #         action = m.sample()\n",
    "    #         target.append(action)\n",
    "            with torch.no_grad():\n",
    "#                 embed_action = gen_action(str(action.item() + 1))  #action : char\n",
    "                embed_action = gen_action(str(action.item()))  #action : char\n",
    "                r, _, _ = env.step(embed_action)\n",
    "#             G_t = gamma*G_t + r\n",
    "            reward_l.append(r)\n",
    "            policy_loss_l.append(-m.log_prob(action))\n",
    "            print(f'{action.item()}:{r}',end = ' ')\n",
    "#         print(f'\\nproba:{prob[0][action.item()]}')\n",
    "        \n",
    "        for k in range(max_episodes_len - 1):\n",
    "            reward_l[max_episodes_len-k-2] = reward_l[max_episodes_len-k-2] + gamma * reward_l[max_episodes_len-k-1]\n",
    "        policy_loss_l = [a * b for a, b in zip(policy_loss_l, reward_l)]\n",
    "\n",
    "        loss = torch.cat(policy_loss_l).sum()  \n",
    "        loss.backward()\n",
    "    #     bat_x = torch.cat(bat_x, 0)\n",
    "    #     target = torch.cat(target, -1)\n",
    "    #     pred, _ = actor(bat_x)\n",
    "    #     log_prob = torch.log(pred)\n",
    "    #     loss = loss_fn(G_t * log_prob, target)\n",
    "\n",
    "    #     loss.backward()\n",
    "    #     sgd(actor.parameters(), 0.1, max_episodes_len)\n",
    "        optimizer.step()\n",
    "\n",
    "#         pr, _= actor(n_state)\n",
    "#         with torch.no_grad():\n",
    "#             pr = act.softmax(pr, dim = 1)\n",
    "#         print(f'after:{pr[0][action.item()]}')\n",
    "        print(f'total reward:{sum(reward_l)}, loss:{loss.data}')\n",
    "\n",
    "end = time.time()\n",
    "end - start\n",
    "# aa = torch.cat(aa, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[182, 166, 222, 259, 288, 256, 124, 371, 220,  42,  59,  93, 279, 180,\n",
       "         205, 359, 274, 348, 292,  46, 229, 164, 339, 202, 330, 263, 272, 153,\n",
       "         231, 236, 347, 337,  17, 376, 298, 364, 285,  77,  57,  47, 144, 283,\n",
       "         187, 345, 130, 344, 270, 133, 131, 346, 241, 223, 191, 305, 117,  14,\n",
       "         273, 320, 299, 257,  11, 178, 149, 212,  48, 331, 217, 173, 333, 278,\n",
       "         363, 159, 171,  13, 225, 227, 342, 271, 374, 119, 327,  43, 282, 190,\n",
       "         341, 141, 353, 318,  55, 167, 261, 176, 194, 322, 151,   5, 295, 201,\n",
       "         281, 326, 138, 314, 352, 186, 158,  97, 198, 302, 122, 310, 199, 356,\n",
       "         163, 291, 316, 268, 293, 373, 313,   6, 154, 349,   3, 248, 160, 235,\n",
       "         311,  54, 336, 362, 325, 192, 262,  67, 324,  60,  25, 204, 379,  35,\n",
       "         290,  24, 104, 360, 372, 246, 183, 267, 177,  38, 238,  44, 269, 206,\n",
       "          40, 224, 103, 354, 109, 312,  83, 116, 251, 143, 328, 100, 155, 214,\n",
       "         179, 157,  45, 213, 211,  22, 355, 209,  32, 309, 140, 208, 369, 169,\n",
       "          33, 258, 226,   2, 332, 101, 321, 343, 335, 249, 266, 245, 319, 189,\n",
       "          18, 102, 193, 297, 152, 218, 255,  15, 260,  72, 174, 242, 315, 156,\n",
       "          70, 148, 358,  21,  51, 351, 161, 338, 264,  86, 250,  58,  26, 118,\n",
       "         317, 126, 215, 280, 142,  62,  75, 106, 340,  95, 145,  50, 188, 252,\n",
       "         139, 240, 137, 370, 185, 306, 366,  30, 162, 232, 350, 289, 221,  98,\n",
       "         105, 234, 276,  61,  28, 300, 123, 323,  88,  94, 203, 244, 284, 247,\n",
       "         301, 200, 170, 239, 378,  64, 329, 243,  81, 380,  63, 172,  84, 286,\n",
       "          91,  16, 367, 150,  65, 216, 368,  96, 275, 184, 253, 308,  82, 108,\n",
       "         254,   8,  78, 128,  53, 110, 132, 134,  92, 381, 307,   9,  52,  85,\n",
       "         168,  99, 136, 230, 197,  10, 129,  89, 210, 107, 296, 181,  41,  23,\n",
       "          87, 304,  79,  73,  76,  66, 135, 228,   0, 165, 147,  31,  69, 195,\n",
       "         120, 334, 115, 237,  74, 377, 111,  34,   4, 175, 125, 114,  29, 265,\n",
       "         357, 287, 365, 219, 375, 277,  90, 361, 303, 196, 233,   1,  80,  39,\n",
       "          36,  19, 113, 294,  71, 146,  56,  37, 207, 112,  20,  49,  12, 127,\n",
       "         121,   7,  27,  68]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_state = np.reshape(train_data.loc[10,'embedded_state'], [-1, state_num, feature_num])\n",
    "n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "prob, _= actor(n_state)\n",
    "prob.sort()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[182, 166, 222, 259, 288, 256, 124, 371, 220,  42,  59,  93, 279, 180,\n",
       "         205, 359, 274, 348, 292,  46, 229, 164, 339, 202, 330, 263, 272, 153,\n",
       "         231, 236, 347, 337,  17, 376, 298, 364, 285,  77,  57,  47, 144, 283,\n",
       "         187, 345, 130, 344, 270, 133, 131, 346, 241, 223, 191, 305, 117,  14,\n",
       "         273, 320, 299, 257,  11, 178, 149, 212,  48, 331, 217, 173, 333, 278,\n",
       "         363, 159, 171,  13, 225, 227, 342, 271, 374, 119, 327,  43, 282, 190,\n",
       "         341, 141, 353, 318,  55, 167, 261, 176, 194, 322, 151,   5, 295, 201,\n",
       "         281, 326, 138, 314, 352, 186, 158,  97, 198, 302, 122, 310, 199, 356,\n",
       "         163, 291, 316, 268, 293, 373, 313,   6, 154, 349,   3, 248, 160, 235,\n",
       "         311,  54, 336, 362, 325, 192, 262,  67, 324,  60,  25, 204, 379,  35,\n",
       "         290,  24, 104, 360, 372, 246, 183, 267, 177,  38, 238,  44, 269, 206,\n",
       "          40, 224, 103, 354, 109, 312,  83, 116, 251, 143, 328, 100, 155, 214,\n",
       "         179, 157,  45, 213, 211,  22, 355, 209,  32, 309, 140, 208, 369, 169,\n",
       "          33, 258, 226,   2, 332, 101, 321, 343, 335, 249, 266, 245, 319, 189,\n",
       "          18, 102, 193, 297, 152, 218, 255,  15, 260,  72, 174, 242, 315, 156,\n",
       "          70, 148,  21, 358,  51, 351, 161, 338, 264,  86, 250,  58,  26, 118,\n",
       "         317, 126, 215, 280, 142,  62,  75, 106, 340,  95, 145,  50, 188, 252,\n",
       "         139, 240, 137, 370, 185, 306, 366,  30, 162, 232, 350, 289, 221,  98,\n",
       "         105, 234, 276,  61,  28, 300, 123, 323,  88,  94, 203, 244, 284, 247,\n",
       "         301, 200, 170, 239, 378,  64, 329, 243,  81, 380,  63, 172,  84, 286,\n",
       "          91,  16, 367, 150,  65, 216, 368,  96, 275, 184, 253, 308,  82, 108,\n",
       "         254,   8,  78, 128,  53, 110, 132, 134,  92, 381, 307,   9,  52,  85,\n",
       "         168,  99, 136, 230, 197,  10, 129,  89, 210, 107, 296, 181,  41,  23,\n",
       "          87, 304,  79,  73,  76,  66, 135, 228,   0, 165, 147,  31,  69, 195,\n",
       "         120, 334, 115, 237,  74, 377, 111,  34,   4, 175, 125, 114,  29, 265,\n",
       "         357, 287, 365, 219, 375, 277,  90, 361, 303, 196, 233,   1,  80,  39,\n",
       "          36,  19, 113, 294,  71, 146,  56,  37, 207, 112,  20,  49,  12, 127,\n",
       "         121,   7,  27,  68]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_state = np.reshape(train_data.loc[99,'embedded_state'], [-1, state_num, feature_num])\n",
    "n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "prob, _= actor(n_state)\n",
    "prob.sort()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.97162306, -0.61097216, -0.72387844, -0.12398621,  1.2741273 ,\n",
       "        -0.44447251, -0.75910548, -0.82978028, -0.61097216, -0.72387844,\n",
       "        -0.12344275,  0.59574275, -0.36199468, -0.75910548,  0.07092139,\n",
       "         2.07791027, -0.72387844, -0.12306521,  1.26318561,  0.36875888,\n",
       "        -0.75910548,  0.07092139,  1.69378421, -0.72387844, -0.10893131,\n",
       "         1.20847718,  0.15651593,  0.72406985,  1.87232473,  0.15727996,\n",
       "        -0.72387844, -0.1105289 ,  2.03639811, -0.49011024, -2.24228081,\n",
       "         0.97162306, -0.61097216, -0.72387844, -0.12460418,  1.43825259,\n",
       "        -0.80352599, -2.24228081,  0.97162306, -0.61097216, -0.72387844,\n",
       "        -0.12244425,  1.31424681, -0.67211132, -2.24228081,  1.87232473,\n",
       "        -0.2268461 , -0.72387844, -0.11831516,  1.46013597, -0.96188342,\n",
       "        -0.75910548,  0.97162306, -0.2268461 , -0.72387844, -0.12726186,\n",
       "         1.0078796 , -0.3652938 , -0.75910548,  0.07092139,  2.8461624 ,\n",
       "        -0.72387844, -0.12359178,  0.93858225, -1.00037308, -0.75910548,\n",
       "         0.97162306,  0.92553209, -0.72387844, -0.12189185,  0.49362034,\n",
       "        -0.50220699,  0.72406985,  0.07092139,  0.15727996, -0.72387844,\n",
       "        -0.12199717,  0.33678951,  0.64863364,  0.72406985]),\n",
       " [104, 51, 78, 196, 29, 22, 24, 125, 120, 85, 212, 184])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[10,'embedded_state'], train_data.loc[10,'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 0.0000e+00, 1.0000e+00, 4.1750e-03, 4.3600e-01,\n",
       "        1.4400e+03, 2.0000e+00, 2.0000e+00, 7.0000e+00, 1.0000e+00,\n",
       "        3.3960e-03, 2.0100e-01, 1.6621e+04, 3.0000e+00, 3.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 7.7120e-03, 7.2100e-01, 9.5400e+02,\n",
       "        1.0000e+00, 4.0000e+00, 0.0000e+00, 1.0000e+00, 4.9990e-03,\n",
       "        6.4500e-01, 5.9600e+02, 1.0000e+00, 4.0000e+00, 4.0000e+00,\n",
       "        1.0000e+00, 2.5387e-02, 8.5100e-01, 2.9500e+02, 1.0000e+00,\n",
       "        4.0000e+00, 7.0000e+00, 1.0000e+00, 6.4600e-03, 5.3200e-01,\n",
       "        1.5570e+03, 2.0000e+00, 2.0000e+00, 7.0000e+00, 1.0000e+00,\n",
       "        6.5770e-03, 6.7400e-01, 2.9540e+03, 1.0000e+00, 2.0000e+00,\n",
       "        7.0000e+00, 1.0000e+00, 6.5770e-03, 6.7400e-01, 2.9540e+03,\n",
       "        1.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00, 1.5764e-02,\n",
       "        8.3300e-01, 2.9540e+03, 1.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "        1.0000e+00, 1.5764e-02, 8.3300e-01, 2.9540e+03, 1.0000e+00,\n",
       "        4.0000e+00, 0.0000e+00, 1.0000e+00, 3.5580e-03, 6.1100e-01,\n",
       "        3.9800e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "        1.5764e-02, 8.3300e-01, 2.9540e+03, 1.0000e+00]),\n",
       " [51, 192, 18, 38, 28, 132, 7, 7, 1, 1, 37, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[1,'embedded_state'], train_data.loc[1,'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0027, 0.0028, 0.0026, 0.0026, 0.0027, 0.0025, 0.0026, 0.0029, 0.0027,\n",
       "         0.0027, 0.0027, 0.0025, 0.0028, 0.0025, 0.0025, 0.0026, 0.0027, 0.0025,\n",
       "         0.0026, 0.0028, 0.0028, 0.0026, 0.0026, 0.0027, 0.0026, 0.0026, 0.0027,\n",
       "         0.0029, 0.0027, 0.0027, 0.0027, 0.0027, 0.0026, 0.0026, 0.0027, 0.0026,\n",
       "         0.0028, 0.0028, 0.0026, 0.0028, 0.0026, 0.0027, 0.0024, 0.0025, 0.0026,\n",
       "         0.0026, 0.0024, 0.0025, 0.0025, 0.0028, 0.0027, 0.0026, 0.0027, 0.0027,\n",
       "         0.0026, 0.0025, 0.0028, 0.0025, 0.0026, 0.0024, 0.0026, 0.0027, 0.0027,\n",
       "         0.0027, 0.0027, 0.0027, 0.0027, 0.0026, 0.0030, 0.0027, 0.0026, 0.0028,\n",
       "         0.0026, 0.0027, 0.0027, 0.0027, 0.0027, 0.0025, 0.0027, 0.0027, 0.0028,\n",
       "         0.0027, 0.0027, 0.0026, 0.0027, 0.0027, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "         0.0028, 0.0027, 0.0027, 0.0024, 0.0027, 0.0027, 0.0027, 0.0026, 0.0027,\n",
       "         0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "         0.0027, 0.0026, 0.0027, 0.0027, 0.0028, 0.0028, 0.0027, 0.0027, 0.0026,\n",
       "         0.0025, 0.0027, 0.0025, 0.0027, 0.0029, 0.0026, 0.0027, 0.0024, 0.0027,\n",
       "         0.0027, 0.0029, 0.0027, 0.0027, 0.0025, 0.0025, 0.0027, 0.0025, 0.0027,\n",
       "         0.0027, 0.0027, 0.0027, 0.0026, 0.0027, 0.0026, 0.0025, 0.0027, 0.0026,\n",
       "         0.0025, 0.0027, 0.0028, 0.0027, 0.0026, 0.0025, 0.0027, 0.0025, 0.0026,\n",
       "         0.0024, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0025, 0.0026, 0.0026,\n",
       "         0.0027, 0.0026, 0.0024, 0.0027, 0.0022, 0.0025, 0.0027, 0.0026, 0.0027,\n",
       "         0.0025, 0.0027, 0.0025, 0.0026, 0.0027, 0.0025, 0.0026, 0.0025, 0.0026,\n",
       "         0.0024, 0.0027, 0.0021, 0.0026, 0.0027, 0.0027, 0.0026, 0.0025, 0.0027,\n",
       "         0.0026, 0.0025, 0.0025, 0.0026, 0.0026, 0.0025, 0.0027, 0.0028, 0.0027,\n",
       "         0.0026, 0.0026, 0.0027, 0.0026, 0.0024, 0.0027, 0.0026, 0.0024, 0.0026,\n",
       "         0.0028, 0.0026, 0.0026, 0.0027, 0.0026, 0.0025, 0.0026, 0.0026, 0.0027,\n",
       "         0.0027, 0.0025, 0.0026, 0.0028, 0.0024, 0.0027, 0.0023, 0.0025, 0.0026,\n",
       "         0.0025, 0.0026, 0.0025, 0.0027, 0.0024, 0.0027, 0.0024, 0.0027, 0.0028,\n",
       "         0.0027, 0.0026, 0.0024, 0.0027, 0.0026, 0.0027, 0.0027, 0.0025, 0.0026,\n",
       "         0.0027, 0.0027, 0.0026, 0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026,\n",
       "         0.0027, 0.0027, 0.0027, 0.0026, 0.0024, 0.0025, 0.0026, 0.0024, 0.0026,\n",
       "         0.0025, 0.0026, 0.0024, 0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026,\n",
       "         0.0025, 0.0025, 0.0024, 0.0025, 0.0024, 0.0027, 0.0027, 0.0028, 0.0025,\n",
       "         0.0024, 0.0027, 0.0026, 0.0025, 0.0025, 0.0027, 0.0025, 0.0027, 0.0028,\n",
       "         0.0024, 0.0027, 0.0026, 0.0026, 0.0024, 0.0026, 0.0028, 0.0026, 0.0027,\n",
       "         0.0026, 0.0025, 0.0025, 0.0027, 0.0027, 0.0026, 0.0028, 0.0027, 0.0025,\n",
       "         0.0027, 0.0027, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026,\n",
       "         0.0026, 0.0026, 0.0027, 0.0025, 0.0026, 0.0025, 0.0026, 0.0025, 0.0027,\n",
       "         0.0026, 0.0026, 0.0026, 0.0025, 0.0026, 0.0027, 0.0024, 0.0025, 0.0026,\n",
       "         0.0025, 0.0027, 0.0026, 0.0026, 0.0025, 0.0026, 0.0024, 0.0027, 0.0025,\n",
       "         0.0025, 0.0026, 0.0025, 0.0025, 0.0025, 0.0025, 0.0024, 0.0026, 0.0027,\n",
       "         0.0026, 0.0026, 0.0025, 0.0026, 0.0026, 0.0026, 0.0028, 0.0026, 0.0024,\n",
       "         0.0026, 0.0028, 0.0026, 0.0025, 0.0025, 0.0028, 0.0027, 0.0027, 0.0027,\n",
       "         0.0026, 0.0027, 0.0024, 0.0026, 0.0026, 0.0025, 0.0028, 0.0025, 0.0027,\n",
       "         0.0027, 0.0026, 0.0027, 0.0027]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 11, 25, 57, 51, 72, 149, 152, 233]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[7,'action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.87232473,  2.07791027, -0.72387844, -0.12089336,  0.78539864,\n",
       "       -0.0766214 ,  0.72406985])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_action('239')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.87232473, -0.61097216, -0.72387844, -0.12432599,  0.73069021,\n",
       "       -0.85796136, -0.75910548])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_action('134')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,1,0,0,0,0,0,0,0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[7,'reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, array([3.33333333e-01, 3.33333333e-01, 0.00000000e+00, 8.35436814e-04,\n",
       "        6.36897767e-01, 1.75459899e-02, 5.00000000e-01, 3.33333333e-01,\n",
       "        6.66666667e-01, 0.00000000e+00, 1.95080676e-03, 7.09753231e-01,\n",
       "        1.35571611e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
       "        0.00000000e+00, 1.95080676e-03, 7.09753231e-01, 1.35571611e-01,\n",
       "        1.00000000e+00, 3.33333333e-01, 6.66666667e-01, 0.00000000e+00,\n",
       "        1.95080676e-03, 7.09753231e-01, 1.35571611e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.11111111e-01, 0.00000000e+00, 9.23104185e-04,\n",
       "        5.19388954e-01, 3.61240969e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.11111111e-01, 0.00000000e+00, 9.23104185e-04, 5.19388954e-01,\n",
       "        3.61240969e-02, 1.00000000e+00, 3.33333333e-01, 2.22222222e-01,\n",
       "        0.00000000e+00, 1.63742707e-03, 9.78848414e-01, 1.70238601e-01,\n",
       "        0.00000000e+00, 3.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.30244215e-03, 9.11868390e-01, 1.76067027e-03, 0.00000000e+00,\n",
       "        6.66666667e-01, 7.77777778e-01, 0.00000000e+00, 6.32991663e-04,\n",
       "        9.17743831e-01, 1.10982940e-01, 0.00000000e+00, 3.33333333e-01,\n",
       "        1.11111111e-01, 0.00000000e+00, 7.90045058e-04, 8.14336075e-01,\n",
       "        4.54131504e-02, 5.00000000e-01, 3.33333333e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.27713931e-04, 5.55816686e-01, 5.40343634e-03,\n",
       "        5.00000000e-01, 0.00000000e+00, 4.44444444e-01, 0.00000000e+00,\n",
       "        4.08629669e-04, 4.38307873e-01, 8.98548965e-02, 5.00000000e-01]), '1,1,1,1,1,1,0,1,0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_state = train_data.loc[0,'embedded_state']\n",
    "action = gen_action('151')\n",
    "r,s,f= env.step(action)\n",
    "r, s, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(),'modelpara.pth')\n",
    "# actor = actor.load_state_dict(torch.load('modelpara.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07092139, -0.61097216, -0.72387844,\n",
       "       -0.11225565,  1.59508343, -0.82771949, -2.24228081])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[7,'embedded_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-161.3674, -155.0946, -156.8075,  ..., -153.8318, -187.5206,\n",
       "         -160.8493],\n",
       "        [-157.4997, -150.8772, -152.9312,  ..., -150.8477, -182.5345,\n",
       "         -156.8122],\n",
       "        [-156.8636, -150.4019, -152.0855,  ..., -150.4251, -181.2172,\n",
       "         -156.2092],\n",
       "        ...,\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119],\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119],\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t * pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([358, 307], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000023CC7FCE480>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(group, x):\n",
    "    max_prob = 0.\n",
    "    nx_size = len(group)\n",
    "    \n",
    "    for i in range(nx_size):\n",
    "        tmp = group.loc[i,'two'] + x\n",
    "        if tmp > max_prob:\n",
    "            max_prob = tmp\n",
    "    print(max_prob)\n",
    "    return max_prob\n",
    "        \n",
    "    \n",
    "pool = ProcessPoolExecutor(max_workers=8)\n",
    "for _ in pool.map(compare, (aa.groupby('one'), 1)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  one              two  three\n",
      "0   1  [1.0, 1.0, 1.0]      3\n",
      "2   1  [0.0, 0.0, 0.0]      5\n",
      "  one              two  three\n",
      "1   2  [0.0, 0.0, 0.0]      4\n",
      "3   2  [1.0, 1.0, 1.0]      8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "one\n",
       "1    2\n",
       "2    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for group in aa.groupby('one'):   \n",
    "    print(group[1])\n",
    "aa.groupby('one').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1.5 1.5 1.5]\n",
      "0\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):   \n",
    "    i = np.argmax(group[1]['three'].values.tolist())\n",
    "    print(i)\n",
    "    print(group[1].iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0.]\n",
      "[0. 3.]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    print(np.dot(np.array(group[1]['two'].values.tolist()), np.ones(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    print(np.array(group[1]['two'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73205081        nan]\n",
      "[       nan 1.73205081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    a = np.array(group[1]['two'].values.tolist())\n",
    "    n = np.linalg.norm(a,2,axis = 1)\n",
    "    d = np.dot(a, np.ones(3))\n",
    "    print(d/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import long_time_task\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.array([[1,2,3],[4,5,6]])\n",
    "np.max(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_2 = np.linalg.norm(np.array(train_data['embedded_action'].values.tolist()), 2, axis = 1)\n",
    "norm_2 = np.where(norm_2 == 0, 0.001, norm_2)\n",
    "norm_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(train_data.loc[0,'embedded_state'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_data['embedded_action'] / norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = train_data['embedded_action'] / norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(bb[5], bb[5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[],2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.info(memory_usage = 'deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['reward'] = train_data['reward'].apply(lambda row: str(row))\n",
    "group_data = train_data.groupby('reward')  \n",
    "for index, data in group_data:\n",
    "    print(type(index))\n",
    "#     print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
