{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231903 entries, 0 to 260086\n",
      "Data columns (total 5 columns):\n",
      "embedded_state     231903 non-null object\n",
      "state              231903 non-null object\n",
      "embedded_action    231903 non-null object\n",
      "action             231903 non-null object\n",
      "reward             231903 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 242.8 MB\n",
      "None\n",
      "20.501777172088623\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "from processing import train_data, embed\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "# from multi_processing import train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class itemEnv():\n",
    "    def __init__(self, alpha = 0.5, sigma = 0.9): # albha parameter in consine similarity\n",
    "        self.observation_space = train_data\n",
    "        self.item_info = embed\n",
    "        self.current_state = None\n",
    "        self.init_state = self.reset()\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.embedding_dim = 7\n",
    "        self.reward_val = 1\n",
    "        self.action_num = 9\n",
    "#         self.rewards, self.avg_state, self.avg_action, self.nx_size = self.avg_group()\n",
    "    \n",
    "    def reset(self):\n",
    "        init_state = self.observation_space['embedded_state'].sample(1).values[0]\n",
    "#         init_state = self.observation_space.loc[0,'embedded_state']\n",
    "        self.current_state = init_state\n",
    "        return init_state\n",
    "\n",
    "#     def feedback(self, pair):  # pair: state-action pair\n",
    "#         prob = list()\n",
    "#         denominator = 0.\n",
    "#         max_prob = 0.\n",
    "#         result = 0.\n",
    "#         feed_back = \"\"\n",
    "#         for r, s, a in zip(self.rewards, self.avg_state, self.avg_action):\n",
    "#             numerator = np.dot(pair[0], s.T) / np.linalg.norm(pair[0], 2) + np.dot(pair[1], a.T) / np.linalg.norm(pair[1], 2)\n",
    "#             denominator += numerator\n",
    "#             prob.append(numerator)\n",
    "#             if numerator > max_prob:\n",
    "#                 max_prob = numerator\n",
    "#                 feed_back = r\n",
    "#         prob = prob/denominator\n",
    "# #         for p, r in zip(prob, self.rewards):\n",
    "# #             for k in range(1):\n",
    "# #                 result += p * self.reward_val * np.power(self.sigma, k) * int(r.split(',')[k])\n",
    "        \n",
    "#         result = self.reward_val * int(feed_back.split(',')[0])\n",
    "#         print(prob)\n",
    "#         return feed_back, result\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(group, pair):\n",
    "        max_prob = 0.\n",
    "        # nx_size = len(group[1])\n",
    "        s_i = np.array(group[1]['embedded_state'].values.tolist()) # 2 dim array:0-->sample, 1-->feature_num (N,(7*12))\n",
    "        s_t = pair[0]                                              # 1 dim array:0-->sample, (7*12,)\n",
    "#         a_i = np.array(group[1]['embedded_action'].values.tolist()) #(N,9,7)\n",
    "        a_i = np.reshape(np.array(group[1]['embedded_action'].values.tolist()),[-1, env.action_num * env.embedding_dim]) #(N, 9*7)\n",
    "        \n",
    "        a_t = np.reshape(pair[1], [env.action_num * env.embedding_dim])                                               #(9*7,)\n",
    "        norm_si = np.linalg.norm(s_i, 2, axis = 1)                 # 1 dim (N,)\n",
    "        norm_st = np.linalg.norm(s_t, 2, axis = 0)                 # 1 dim (1,)\n",
    "        norm_ai = np.linalg.norm(a_i, 2, axis = 1)                 #1 dim (N,)\n",
    "        norm_at = np.linalg.norm(a_t, 2, axis = 0)                 # 1 dim (1,)\n",
    "        \n",
    "        #first term: (N,), second term: (N,)\n",
    "        cos = env.alpha * (np.dot(s_i, s_t)/(norm_si * norm_st + 1e-10)) + (1 - env.alpha) * np.dot(a_i, a_t)/(norm_ai * norm_at + 1e-10)  #1 dim\n",
    "        # cos = cos / np.sum(cos)\n",
    "        #cos: (N,)\n",
    "        # max_id = np.argmax(cos)\n",
    "#         id_x = int(max_id / cos.shape[1])\n",
    "#         id_y = max_id % cos.shape[1]\n",
    "        # print('i am here!')\n",
    "        return np.max(cos)\n",
    "    \n",
    "    def feedback_2(self, pair):\n",
    "        data = train_data.groupby(['reward'])\n",
    "#         denominator = 0.\n",
    "        reward_ind = -1\n",
    "        max_prob = 0.\n",
    "        result = 0.\n",
    "        feed_back = \"\"\n",
    "        fnc = partial(self.cos_sim, pair = pair)\n",
    "        prob = map(fnc, data)\n",
    "        r = 0.\n",
    "        for tup, group in zip(prob, data):\n",
    "            if tup > max_prob:\n",
    "                max_prob = tup\n",
    "#                 reward_ind = tup[2]  #id_y\n",
    "                feed_back = group[0]          #index: reward str\n",
    "#                 feed_back = group[1].iloc[tup[1], 2]\n",
    "#                 idx = tup[1]\n",
    "        for k, reward in enumerate(feed_back.split(',')):\n",
    "            r += max_prob * np.power(self.sigma, k) * (-1 if reward == \"0\" else 1)\n",
    "            \n",
    "#         r = 1 * int(feed_back.split(','))  #reward_val\n",
    "#         result =  r if r > 0 else -1\n",
    "\n",
    "        return feed_back, r, max_prob\n",
    "            \n",
    "\n",
    "    def step(self, action):   #action: 2 dim array\n",
    "        feed_back, result, p = self.feedback_2((self.current_state, action))\n",
    "#         for i,r in enumerate([feed_back.split(',')[0]]): #0---> one action\n",
    "        for i, r in enumerate(feed_back.split(',')):\n",
    "            if r != \"0\":\n",
    "                # self.current_state.append(action[i])\n",
    "                tmp = np.append(self.current_state, action[i,:])\n",
    "#                 tmp = np.delete(tmp, 0, axis=0)\n",
    "                tmp = tmp[self.embedding_dim:]\n",
    "                self.current_state = tmp\n",
    "\n",
    "#         if result == 1:     #reward_val\n",
    "#             tmp = np.append(self.current_state, action)\n",
    "#             tmp = tmp[self.embedding_dim:]\n",
    "#             self.current_state = tmp  # state: 1 dim array\n",
    "# #         else:\n",
    "# #             self.current_state = self.observation_space['embedded_state'].sample(1).values[0]\n",
    "\n",
    "        return result, self.current_state, feed_back, p\n",
    "\n",
    "#     def avg_group(self):\n",
    "#         nx_size = list()\n",
    "#         avg_state = list()\n",
    "#         avg_action = list()\n",
    "#         rewards = list()\n",
    "#         for reward, group in self.observation_space.groupby(['reward']):\n",
    "#             nx_size.append(group.shape[0])\n",
    "# #             state = np.mean(data['embedded_action'], axis = 0)\n",
    "#             norm_s = np.linalg.norm(np.array(group['embedded_state'].values.tolist()), 2, axis = 1)\n",
    "#             norm_s = np.where(norm_s == 0, 0.001, norm_s)\n",
    "#             state = np.sum(group['embedded_state'] / norm_s) / group.shape[0]\n",
    "#             norm_a = np.linalg.norm(np.array(group['embedded_action'].values.tolist()), 2, axis = 1)\n",
    "#             norm_a = np.where(norm_a == 0, 0.001, norm_a)\n",
    "#             action = np.sum(group['embedded_action'] / norm_a) / group.shape[0]\n",
    "#             avg_state.append(state)\n",
    "#             avg_action.append(action)\n",
    "#             rewards.append(reward)\n",
    "\n",
    "#         return rewards, avg_state, avg_action, nx_size\n",
    "def gen_action(item_id):\n",
    "    return embed[item_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.87232473, -0.2268461 , -0.72387844, -0.11876125,  0.61762612,\n",
       "       -0.74414196,  0.72406985,  1.87232473, -0.61097216, -0.72387844,\n",
       "       -0.11926497,  0.36961457, -0.77438383,  0.72406985, -0.82978028,\n",
       "        1.69378421, -0.72387844, -0.10210977,  0.60668444,  0.92300988,\n",
       "        0.72406985,  0.07092139, -0.61097216, -0.72387844, -0.1151329 ,\n",
       "        1.83580053, -1.0553583 , -2.24228081,  0.97162306, -0.61097216,\n",
       "       -0.72387844, -0.12482077,  1.38354416, -0.80352599, -2.24228081,\n",
       "        0.97162306, -0.61097216, -0.72387844, -0.12244425,  1.31424681,\n",
       "       -0.67211132, -2.24228081,  0.07092139, -0.2268461 , -0.72387844,\n",
       "       -0.12003396,  1.53308054, -0.66001457, -0.75910548,  1.87232473,\n",
       "       -0.61097216, -0.72387844, -0.12301752,  1.02611574, -0.47086542,\n",
       "       -0.75910548,  0.97162306,  2.07791027, -0.72387844, -0.12267674,\n",
       "        1.15376875, -0.17504494, -0.75910548,  1.87232473,  0.92553209,\n",
       "       -0.72387844, -0.11692224,  0.96776008, -0.8805053 ,  0.72406985,\n",
       "        0.97162306, -0.61097216, -0.72387844, -0.12460815, -0.25406155,\n",
       "       -0.63527122,  0.72406985, -0.82978028,  1.69378421, -0.72387844,\n",
       "       -0.10210977,  0.60668444,  0.92300988,  0.72406985])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = itemEnv()\n",
    "env.reset()\n",
    "env.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.968528482739641,\n",
       " array([ 1.87232473, -0.61097216, -0.72387844, -0.12301752,  1.02611574,\n",
       "        -0.47086542, -0.75910548,  0.97162306,  2.07791027, -0.72387844,\n",
       "        -0.12267674,  1.15376875, -0.17504494, -0.75910548,  1.87232473,\n",
       "         0.92553209, -0.72387844, -0.11692224,  0.96776008, -0.8805053 ,\n",
       "         0.72406985,  0.97162306, -0.61097216, -0.72387844, -0.12460815,\n",
       "        -0.25406155, -0.63527122,  0.72406985, -0.82978028,  1.69378421,\n",
       "        -0.72387844, -0.10210977,  0.60668444,  0.92300988,  0.72406985,\n",
       "        -0.82978028, -0.61097216,  1.38144741, -0.12759072, -0.99444899,\n",
       "         0.60574517,  0.72406985,  0.07092139, -0.2268461 , -0.72387844,\n",
       "        -0.12688829, -0.37806733, -0.57588719,  0.72406985, -0.82978028,\n",
       "         2.07791027, -0.72387844, -0.12561956, -0.87409044,  1.99027298,\n",
       "         0.72406985, -0.82978028, -0.61097216, -0.72387844, -0.12327285,\n",
       "         0.7926931 , -0.9563849 , -0.75910548,  0.97162306, -0.2268461 ,\n",
       "        -0.72387844, -0.12246114,  0.35867288, -0.50055744,  0.72406985,\n",
       "         0.07092139,  1.69378421, -0.72387844, -0.11133664,  1.43460536,\n",
       "         0.77070083,  0.72406985, -0.82978028, -0.61097216,  1.38144741,\n",
       "        -0.12759072, -0.99444899, -0.71390009,  0.72406985]),\n",
       " '1,1,1,1,1,1,1,0,0',\n",
       " 0.45691888014385323,\n",
       " 0.5959303379058838)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "# action = train_data['embedded_action'].sample(1).values[0][5]\n",
    "# action = train_data.loc[0,'embedded_action'][0]\n",
    "action_l = []\n",
    "for i in range(9):\n",
    "    action_l.append(str(randint(1, 381)))\n",
    "\n",
    "action = np.array(list(map(gen_action, action_l)))\n",
    "r,s,f,p = env.step(action)\n",
    "end = time.time()\n",
    "r, s, f,p,end - start\n",
    "# action, action_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "action_l = []\n",
    "for i in range(1):\n",
    "    action_l.append(str(randint(1, 381)))\n",
    "\n",
    "action = np.array(list(map(gen_action, action_l)))\n",
    "r,s,f,p = env.step(action)\n",
    "end = time.time()\n",
    "r,s,f,p,end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward\n",
       "0,0,0,0,0,0,0,0,0    17548\n",
       "0,0,1,0,0,0,0,0,0     4749\n",
       "0,1,0,0,0,0,0,0,0     4576\n",
       "0,1,1,0,0,0,0,0,0     2487\n",
       "1,0,0,0,0,0,0,0,0     5481\n",
       "1,0,1,0,0,0,0,0,0     2662\n",
       "1,1,0,0,0,0,0,0,0     3555\n",
       "1,1,0,0,1,0,0,0,1        1\n",
       "1,1,1,0,0,0,0,0,0     3918\n",
       "1,1,1,0,0,1,0,0,0     3644\n",
       "1,1,1,0,1,0,0,0,0     3557\n",
       "1,1,1,0,1,1,0,0,0     2806\n",
       "1,1,1,1,0,0,0,0,0     4175\n",
       "1,1,1,1,0,1,0,0,0     2983\n",
       "1,1,1,1,1,0,0,0,0     4286\n",
       "1,1,1,1,1,1,0,0,0     2877\n",
       "1,1,1,1,1,1,0,0,1     4394\n",
       "1,1,1,1,1,1,0,1,0     7328\n",
       "1,1,1,1,1,1,0,1,1     6496\n",
       "1,1,1,1,1,1,1,0,0     9277\n",
       "1,1,1,1,1,1,1,0,1     8031\n",
       "1,1,1,1,1,1,1,1,0    31676\n",
       "1,1,1,1,1,1,1,1,1    25825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('reward').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as act\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu():\n",
    "    i = 0\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, inpt_sz, hidden_sz, hidden_l1, oupt_sz): #embedding dim, , , 381\n",
    "        super().__init__()\n",
    "        #input_gate\n",
    "        device = try_gpu()\n",
    "        init_weights = partial(self.init_, device = device)\n",
    "#         self.init_state = None\n",
    "        self.reward = []\n",
    "        self.inpt_sz = inpt_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.hidden_l1 = hidden_l1\n",
    "        self. oupt_sz = oupt_sz\n",
    "        self.W_xi, self.W_hi, self.bias_i = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #forget_gate\n",
    "        self.W_xf, self.W_hf, self.bias_f = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #output_gate\n",
    "        self.W_xo, self.W_ho, self.bias_o = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #candidate memory cell\n",
    "        self.W_xc, self.W_hc, self.bias_c = (init_weights((inpt_sz, hidden_sz)), init_weights(\n",
    "                                            (hidden_sz, hidden_sz)), init_weights((1, hidden_sz))\n",
    "                                            )\n",
    "        #output_layer\n",
    "        self.W_o1, self.W_o2, self.bias_o1, self.bias_o2 = (init_weights((hidden_sz, hidden_l1)), init_weights(\n",
    "                                            (hidden_l1, oupt_sz)), init_weights((1, hidden_l1)), \n",
    "                                            init_weights((1, oupt_sz))\n",
    "                                            ) \n",
    "    @staticmethod\n",
    "    def init_(shape, device):\n",
    "#         param = torch.tensor(shape)\n",
    "        def xvaier(param):\n",
    "            return nn.init.xavier_uniform_(param)\n",
    "        param = xvaier(torch.rand(shape, device = device))\n",
    "        return nn.Parameter(param)\n",
    "    \n",
    "    def forward(self, X, init_state = None):  #X: batch size, seq size, input size\n",
    "        batch_size, seq_size, _ = X.shape\n",
    "        hidden_sz = self.hidden_sz\n",
    "#         oupts = []\n",
    "        if init_state == None:         #H, C actually are constant, not trainable\n",
    "            H, C = (torch.zeros(batch_size, hidden_sz, device = X.device),\n",
    "                    torch.zeros(batch_size, hidden_sz, device = X.device)\n",
    "                   )\n",
    "        else:\n",
    "            H, C = init_state    # in some circumstance\n",
    "#         softmax = act.softmax(dim = 1)\n",
    "        for seq in range(seq_size):\n",
    "            x_t = X[:,seq,:]\n",
    "            I = torch.sigmoid(x_t @ self.W_xi + H @ self.W_hi + self.bias_i)\n",
    "            F = torch.sigmoid(x_t @ self.W_xf + H @ self.W_hf + self.bias_f)\n",
    "            O = torch.sigmoid(x_t @ self.W_xo + H @ self.W_ho + self.bias_o)\n",
    "            C_tilda = torch.tanh(x_t @ self.W_xc + H @ self.W_hc + self.bias_c)\n",
    "            \n",
    "            C = F * C + I * C_tilda\n",
    "            H = torch.tanh(C) + O\n",
    "            hidden_layer1 = torch.relu(H @ self.W_o1 + self.bias_o1)\n",
    "            output = act.softmax(hidden_layer1 @ self.W_o2 + self.bias_o2, dim = 1)\n",
    "#             oupts.append(output)\n",
    "            \n",
    "#         self.init_state = (H, C)\n",
    "#         return torch.cat(oupts, dim = 0)\n",
    "        return output, (H, C)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = LSTM(7, 256, 256, 381)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([182, 330, 336], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_state = np.reshape(env.current_state, [-1, 12, 7])\n",
    "n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "# bat_x.append(n_state)\n",
    "prob, _= actor(n_state)\n",
    "m = Categorical(prob)\n",
    "m.sample((3,)).squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-cdc9f71c323c>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-65-cdc9f71c323c>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    if action == action_l\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def action_list(m):\n",
    "    action_l = []\n",
    "    action_l.append(str(m.sample().item()))\n",
    "    for i in range(1, 9, 1):\n",
    "        while True:\n",
    "            action = str(m.sample.item())\n",
    "            if action == action_l:\n",
    "        action_l.append(str(m.sample.item()))\n",
    "\n",
    "    actions = np.array(list(map(gen_action, action_l)))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward:-2.2727415814225926, loss:-175.91790771484375\n",
      "total reward:-3.0435854883553275, loss:0.32864493131637573\n",
      "total reward:-4.700932837244929, loss:-4.910978532279842e-06\n",
      "total reward:-4.700932837244929, loss:-4.910978532279842e-06\n",
      "total reward:-3.7985602083553283, loss:6.192017281136941e-07\n",
      "total reward:-4.453542720995328, loss:-2.6447573873156216e-06\n",
      "total reward:-4.281744029155329, loss:-1.5345842712122248e-06\n",
      "total reward:-4.453542720995328, loss:-2.6447573873156216e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-510be38fc54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#         target.append(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0membed_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#         actor.reward.append(r)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mG_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mG_t\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7b6eba78c837>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#action: 1 dim array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mfeed_back\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedback_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;31m#         for i,r in enumerate([feed_back.split(',')[0]]): #0---> one action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[1;31m#reward_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7b6eba78c837>\u001b[0m in \u001b[0;36mfeedback_2\u001b[1;34m(self, pair)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mfnc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mmax_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7b6eba78c837>\u001b[0m in \u001b[0;36mcos_sim\u001b[1;34m(group, pair)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0ms_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedded_state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 2 dim array:0-->sample, 1-->feature_num (N,(7*12))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0ms_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m                                              \u001b[1;31m# 1 dim array:0-->sample, (7*12,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0ma_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedded_action'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(N,9,7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0ma_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m                                               \u001b[1;31m#(7,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mnorm_si\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# 1 dim (N,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "# def maximumll(inpt, label):\n",
    "    \n",
    "start = time.time()\n",
    "    \n",
    "loss_fn = nn.NLLLoss(reduction='sum')\n",
    "max_episodes = 100\n",
    "max_episodes_len = 16\n",
    "gamma = 0.8\n",
    "optimizer = torch.optim.SGD(actor.parameters(), lr = 0.1)\n",
    "for episode in range(max_episodes):\n",
    "    env.reset()\n",
    "    G_t = 0.\n",
    "    bat_x = []\n",
    "    target = []\n",
    "    policy_loss_l = []\n",
    "#     del actor.reward[:]\n",
    "    for i in range(max_episodes_len):\n",
    "        n_state = np.reshape(env.current_state, [-1, 12, 7])\n",
    "        n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "        bat_x.append(n_state)\n",
    "        prob, _= actor(n_state)\n",
    "        \n",
    "        m = Categorical(prob)\n",
    "\n",
    "#         action = m.sample()\n",
    "        actions = m.sample((9,)).squeeze(dim = 1)\n",
    "#         target.append(action)\n",
    "        embed_action = gen_action(str(action.item()))\n",
    "        r, _, _ = env.step(embed_action)\n",
    "#         actor.reward.append(r)\n",
    "        G_t = gamma*G_t + r\n",
    "        policy_loss_l.append(-m.log_prob(action) * G_t)\n",
    "    \n",
    "#     bat_x = torch.cat(bat_x, 0)\n",
    "#     target = torch.cat(target, -1)\n",
    "    loss = torch.cat(policy_loss_l).sum()  \n",
    "    loss.backward()\n",
    "#     pred, _ = actor(bat_x)\n",
    "#     log_prob = torch.log(pred)\n",
    "#     loss = loss_fn(G_t * log_prob, target)\n",
    "    \n",
    "#     loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'total reward:{G_t}, loss:{loss.data}')\n",
    "\n",
    "end = time.time()\n",
    "end - start\n",
    "# aa = torch.cat(aa, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.4076]]), tensor(2.4076))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "a = torch.Tensor([[1,2,3]])\n",
    "b = torch.Tensor([[1, 0, 0]])\n",
    "c = torch.Tensor([0]).long()\n",
    "\n",
    "o = act.softmax(a, dim = 1)\n",
    "y = -o.log() @ b.T\n",
    "p = loss(o.log(), c)\n",
    "y,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(35, device='cuda:0'),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = train_data.loc[0,'embedded_state']\n",
    "n_state = np.reshape(s, [-1, 12, 7])\n",
    "n_state = torch.from_numpy(n_state).cuda().to(torch.float32)\n",
    "p,_=actor(n_state)\n",
    "torch.argmax(p),p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 28, 20, 84, 105, 106, 239, 212, 151]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0,'action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0013, 0.0010, 0.0039, 0.0020, 0.0048, 0.0019, 0.0030, 0.0011, 0.0011,\n",
       "         0.0051, 0.0008, 0.0033, 0.0086, 0.0026, 0.0011, 0.0008, 0.0040, 0.0034,\n",
       "         0.0029, 0.0018, 0.0053, 0.0014, 0.0024, 0.0010, 0.0013, 0.0018, 0.0069,\n",
       "         0.0005, 0.0012, 0.0030, 0.0027, 0.0023, 0.0013, 0.0024, 0.0019, 0.0024,\n",
       "         0.0016, 0.0057, 0.0013, 0.0007, 0.0094, 0.0005, 0.0023, 0.0015, 0.0034,\n",
       "         0.0050, 0.0019, 0.0018, 0.0071, 0.0021, 0.0023, 0.0024, 0.0014, 0.0010,\n",
       "         0.0029, 0.0007, 0.0020, 0.0045, 0.0032, 0.0016, 0.0013, 0.0019, 0.0033,\n",
       "         0.0081, 0.0018, 0.0013, 0.0022, 0.0028, 0.0023, 0.0014, 0.0029, 0.0018,\n",
       "         0.0021, 0.0034, 0.0022, 0.0021, 0.0016, 0.0011, 0.0008, 0.0029, 0.0036,\n",
       "         0.0026, 0.0064, 0.0030, 0.0009, 0.0018, 0.0030, 0.0063, 0.0027, 0.0020,\n",
       "         0.0032, 0.0022, 0.0044, 0.0040, 0.0008, 0.0016, 0.0056, 0.0006, 0.0030,\n",
       "         0.0017, 0.0042, 0.0071, 0.0014, 0.0034, 0.0027, 0.0077, 0.0007, 0.0013,\n",
       "         0.0027, 0.0023, 0.0017, 0.0022, 0.0005, 0.0023, 0.0025, 0.0062, 0.0009,\n",
       "         0.0012, 0.0023, 0.0037, 0.0017, 0.0015, 0.0035, 0.0009, 0.0013, 0.0036,\n",
       "         0.0060, 0.0013, 0.0013, 0.0006, 0.0012, 0.0063, 0.0024, 0.0023, 0.0015,\n",
       "         0.0052, 0.0037, 0.0013, 0.0012, 0.0017, 0.0015, 0.0025, 0.0016, 0.0069,\n",
       "         0.0038, 0.0036, 0.0009, 0.0007, 0.0008, 0.0019, 0.0012, 0.0014, 0.0021,\n",
       "         0.0118, 0.0020, 0.0004, 0.0024, 0.0027, 0.0008, 0.0034, 0.0076, 0.0021,\n",
       "         0.0011, 0.0019, 0.0034, 0.0042, 0.0037, 0.0048, 0.0016, 0.0013, 0.0023,\n",
       "         0.0013, 0.0017, 0.0018, 0.0041, 0.0016, 0.0020, 0.0010, 0.0029, 0.0039,\n",
       "         0.0018, 0.0012, 0.0022, 0.0013, 0.0010, 0.0024, 0.0013, 0.0012, 0.0022,\n",
       "         0.0022, 0.0019, 0.0014, 0.0022, 0.0011, 0.0097, 0.0098, 0.0028, 0.0008,\n",
       "         0.0022, 0.0020, 0.0032, 0.0012, 0.0037, 0.0031, 0.0022, 0.0031, 0.0018,\n",
       "         0.0022, 0.0035, 0.0024, 0.0014, 0.0036, 0.0022, 0.0014, 0.0012, 0.0014,\n",
       "         0.0011, 0.0035, 0.0028, 0.0016, 0.0021, 0.0038, 0.0023, 0.0017, 0.0038,\n",
       "         0.0015, 0.0025, 0.0009, 0.0043, 0.0031, 0.0023, 0.0027, 0.0007, 0.0023,\n",
       "         0.0014, 0.0032, 0.0034, 0.0007, 0.0045, 0.0026, 0.0010, 0.0010, 0.0024,\n",
       "         0.0028, 0.0028, 0.0009, 0.0017, 0.0021, 0.0021, 0.0035, 0.0014, 0.0022,\n",
       "         0.0007, 0.0014, 0.0027, 0.0034, 0.0033, 0.0021, 0.0037, 0.0005, 0.0019,\n",
       "         0.0026, 0.0033, 0.0005, 0.0029, 0.0016, 0.0044, 0.0016, 0.0143, 0.0014,\n",
       "         0.0016, 0.0029, 0.0034, 0.0037, 0.0039, 0.0023, 0.0010, 0.0021, 0.0017,\n",
       "         0.0059, 0.0035, 0.0035, 0.0077, 0.0027, 0.0029, 0.0023, 0.0011, 0.0009,\n",
       "         0.0006, 0.0023, 0.0033, 0.0016, 0.0050, 0.0018, 0.0028, 0.0056, 0.0016,\n",
       "         0.0025, 0.0028, 0.0014, 0.0029, 0.0031, 0.0024, 0.0011, 0.0023, 0.0015,\n",
       "         0.0054, 0.0028, 0.0021, 0.0010, 0.0025, 0.0016, 0.0010, 0.0076, 0.0016,\n",
       "         0.0032, 0.0018, 0.0016, 0.0007, 0.0027, 0.0018, 0.0010, 0.0030, 0.0034,\n",
       "         0.0017, 0.0031, 0.0009, 0.0019, 0.0032, 0.0028, 0.0019, 0.0013, 0.0020,\n",
       "         0.0014, 0.0040, 0.0029, 0.0022, 0.0027, 0.0016, 0.0029, 0.0041, 0.0014,\n",
       "         0.0065, 0.0013, 0.0060, 0.0019, 0.0047, 0.0026, 0.0059, 0.0016, 0.0077,\n",
       "         0.0020, 0.0027, 0.0020, 0.0003, 0.0014, 0.0010, 0.0009, 0.0039, 0.0023,\n",
       "         0.0012, 0.0029, 0.0026, 0.0026, 0.0150, 0.0028, 0.0045, 0.0020, 0.0020,\n",
       "         0.0027, 0.0017, 0.0019, 0.0015, 0.0043, 0.0007, 0.0024, 0.0029, 0.0027,\n",
       "         0.0014, 0.0007, 0.0009],\n",
       "        [0.0013, 0.0010, 0.0039, 0.0020, 0.0048, 0.0019, 0.0030, 0.0011, 0.0011,\n",
       "         0.0051, 0.0008, 0.0033, 0.0086, 0.0026, 0.0011, 0.0008, 0.0040, 0.0034,\n",
       "         0.0029, 0.0018, 0.0053, 0.0014, 0.0024, 0.0010, 0.0013, 0.0018, 0.0069,\n",
       "         0.0005, 0.0012, 0.0030, 0.0027, 0.0023, 0.0013, 0.0024, 0.0019, 0.0024,\n",
       "         0.0016, 0.0057, 0.0013, 0.0007, 0.0094, 0.0005, 0.0023, 0.0015, 0.0034,\n",
       "         0.0050, 0.0019, 0.0018, 0.0071, 0.0021, 0.0023, 0.0024, 0.0014, 0.0010,\n",
       "         0.0029, 0.0007, 0.0020, 0.0045, 0.0032, 0.0016, 0.0013, 0.0019, 0.0033,\n",
       "         0.0081, 0.0018, 0.0013, 0.0022, 0.0028, 0.0023, 0.0014, 0.0029, 0.0018,\n",
       "         0.0021, 0.0034, 0.0022, 0.0021, 0.0016, 0.0011, 0.0008, 0.0029, 0.0036,\n",
       "         0.0026, 0.0064, 0.0030, 0.0009, 0.0018, 0.0030, 0.0063, 0.0027, 0.0020,\n",
       "         0.0032, 0.0022, 0.0044, 0.0040, 0.0008, 0.0016, 0.0056, 0.0006, 0.0030,\n",
       "         0.0017, 0.0042, 0.0071, 0.0014, 0.0034, 0.0027, 0.0077, 0.0007, 0.0013,\n",
       "         0.0027, 0.0023, 0.0017, 0.0022, 0.0005, 0.0023, 0.0025, 0.0062, 0.0009,\n",
       "         0.0012, 0.0023, 0.0037, 0.0017, 0.0015, 0.0035, 0.0009, 0.0013, 0.0036,\n",
       "         0.0060, 0.0013, 0.0013, 0.0006, 0.0012, 0.0063, 0.0024, 0.0023, 0.0015,\n",
       "         0.0052, 0.0037, 0.0013, 0.0012, 0.0017, 0.0015, 0.0025, 0.0016, 0.0069,\n",
       "         0.0038, 0.0036, 0.0009, 0.0007, 0.0008, 0.0019, 0.0012, 0.0014, 0.0021,\n",
       "         0.0118, 0.0020, 0.0004, 0.0024, 0.0027, 0.0008, 0.0034, 0.0076, 0.0021,\n",
       "         0.0011, 0.0019, 0.0034, 0.0042, 0.0037, 0.0048, 0.0016, 0.0013, 0.0023,\n",
       "         0.0013, 0.0017, 0.0018, 0.0041, 0.0016, 0.0020, 0.0010, 0.0029, 0.0039,\n",
       "         0.0018, 0.0012, 0.0022, 0.0013, 0.0010, 0.0024, 0.0013, 0.0012, 0.0022,\n",
       "         0.0022, 0.0019, 0.0014, 0.0022, 0.0011, 0.0097, 0.0098, 0.0028, 0.0008,\n",
       "         0.0022, 0.0020, 0.0032, 0.0012, 0.0037, 0.0031, 0.0022, 0.0031, 0.0018,\n",
       "         0.0022, 0.0035, 0.0024, 0.0014, 0.0036, 0.0022, 0.0014, 0.0012, 0.0014,\n",
       "         0.0011, 0.0035, 0.0028, 0.0016, 0.0021, 0.0038, 0.0023, 0.0017, 0.0038,\n",
       "         0.0015, 0.0025, 0.0009, 0.0043, 0.0031, 0.0023, 0.0027, 0.0007, 0.0023,\n",
       "         0.0014, 0.0032, 0.0034, 0.0007, 0.0045, 0.0026, 0.0010, 0.0010, 0.0024,\n",
       "         0.0028, 0.0028, 0.0009, 0.0017, 0.0021, 0.0021, 0.0035, 0.0014, 0.0022,\n",
       "         0.0007, 0.0014, 0.0027, 0.0034, 0.0033, 0.0021, 0.0037, 0.0005, 0.0019,\n",
       "         0.0026, 0.0033, 0.0005, 0.0029, 0.0016, 0.0044, 0.0016, 0.0143, 0.0014,\n",
       "         0.0016, 0.0029, 0.0034, 0.0037, 0.0039, 0.0023, 0.0010, 0.0021, 0.0017,\n",
       "         0.0059, 0.0035, 0.0035, 0.0077, 0.0027, 0.0029, 0.0023, 0.0011, 0.0009,\n",
       "         0.0006, 0.0023, 0.0033, 0.0016, 0.0050, 0.0018, 0.0028, 0.0056, 0.0016,\n",
       "         0.0025, 0.0028, 0.0014, 0.0029, 0.0031, 0.0024, 0.0011, 0.0023, 0.0015,\n",
       "         0.0054, 0.0028, 0.0021, 0.0010, 0.0025, 0.0016, 0.0010, 0.0076, 0.0016,\n",
       "         0.0032, 0.0018, 0.0016, 0.0007, 0.0027, 0.0018, 0.0010, 0.0030, 0.0034,\n",
       "         0.0017, 0.0031, 0.0009, 0.0019, 0.0032, 0.0028, 0.0019, 0.0013, 0.0020,\n",
       "         0.0014, 0.0040, 0.0029, 0.0022, 0.0027, 0.0016, 0.0029, 0.0041, 0.0014,\n",
       "         0.0065, 0.0013, 0.0060, 0.0019, 0.0047, 0.0026, 0.0059, 0.0016, 0.0077,\n",
       "         0.0020, 0.0027, 0.0020, 0.0003, 0.0014, 0.0010, 0.0009, 0.0039, 0.0023,\n",
       "         0.0012, 0.0029, 0.0026, 0.0026, 0.0150, 0.0028, 0.0045, 0.0020, 0.0020,\n",
       "         0.0027, 0.0017, 0.0019, 0.0015, 0.0043, 0.0007, 0.0024, 0.0029, 0.0027,\n",
       "         0.0014, 0.0007, 0.0009]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-161.3674, -155.0946, -156.8075,  ..., -153.8318, -187.5206,\n",
       "         -160.8493],\n",
       "        [-157.4997, -150.8772, -152.9312,  ..., -150.8477, -182.5345,\n",
       "         -156.8122],\n",
       "        [-156.8636, -150.4019, -152.0855,  ..., -150.4251, -181.2172,\n",
       "         -156.2092],\n",
       "        ...,\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119],\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119],\n",
       "        [-155.1541, -148.4594, -150.3322,  ..., -148.8944, -179.0215,\n",
       "         -154.5119]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t * pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([358, 307], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000023CC7FCE480>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(group, x):\n",
    "    max_prob = 0.\n",
    "    nx_size = len(group)\n",
    "    \n",
    "    for i in range(nx_size):\n",
    "        tmp = group.loc[i,'two'] + x\n",
    "        if tmp > max_prob:\n",
    "            max_prob = tmp\n",
    "    print(max_prob)\n",
    "    return max_prob\n",
    "        \n",
    "    \n",
    "pool = ProcessPoolExecutor(max_workers=8)\n",
    "for _ in pool.map(compare, (aa.groupby('one'), 1)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  one              two  three\n",
      "0   1  [1.0, 1.0, 1.0]      3\n",
      "2   1  [0.0, 0.0, 0.0]      5\n",
      "  one              two  three\n",
      "1   2  [0.0, 0.0, 0.0]      4\n",
      "3   2  [1.0, 1.0, 1.0]      8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "one\n",
       "1    2\n",
       "2    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for group in aa.groupby('one'):   \n",
    "    print(group[1])\n",
    "aa.groupby('one').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1.5 1.5 1.5]\n",
      "0\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):   \n",
    "    i = np.argmax(group[1]['three'].values.tolist())\n",
    "    print(i)\n",
    "    print(group[1].iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0.]\n",
      "[0. 3.]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    print(np.dot(np.array(group[1]['two'].values.tolist()), np.ones(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    print(np.array(group[1]['two'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73205081        nan]\n",
      "[       nan 1.73205081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for group in aa.groupby('one'):\n",
    "    a = np.array(group[1]['two'].values.tolist())\n",
    "    n = np.linalg.norm(a,2,axis = 1)\n",
    "    d = np.dot(a, np.ones(3))\n",
    "    print(d/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import long_time_task\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.array([[1,2,3],[4,5,6]])\n",
    "np.max(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_2 = np.linalg.norm(np.array(train_data['embedded_action'].values.tolist()), 2, axis = 1)\n",
    "norm_2 = np.where(norm_2 == 0, 0.001, norm_2)\n",
    "norm_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(train_data.loc[0,'embedded_state'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_data['embedded_action'] / norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = train_data['embedded_action'] / norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(bb[5], bb[5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[],2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.info(memory_usage = 'deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['reward'] = train_data['reward'].apply(lambda row: str(row))\n",
    "group_data = train_data.groupby('reward')  \n",
    "for index, data in group_data:\n",
    "    print(type(index))\n",
    "#     print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
