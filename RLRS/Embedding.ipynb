{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_win(row, window_sz = 6, skip_num = 1):\n",
    "    group = ceil((len(row) - window_sz + 1)/skip_num)\n",
    "    j = 0\n",
    "    sample = []\n",
    "#     label = []\n",
    "    for i in range(group):\n",
    "        sample.append(row[j:j + window_sz - 1]) \n",
    "#         label.append(row[j + window_sz - 1])\n",
    "        j += skip_num\n",
    "\n",
    "    return np.array(sample)\n",
    "\n",
    "def sliding_win_lab(row, window_sz = 6, skip_num = 1):\n",
    "    group = ceil((len(row) - window_sz + 1)/skip_num)\n",
    "    j = 0\n",
    "#     sample = []\n",
    "    label = []\n",
    "    for i in range(group):\n",
    "#         sample.append(row[j:j + window_sz - 1]) \n",
    "        label.append(row[j + window_sz - 1])\n",
    "        j += skip_num\n",
    "\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_click_history</th>\n",
       "      <th>user_protrait</th>\n",
       "      <th>exposed_items</th>\n",
       "      <th>labels</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30:1580603130,34:1581178937,15:1581178939,28:1...</td>\n",
       "      <td>64054,21804,80911,36504,8867,7615,54240,37294,...</td>\n",
       "      <td>1,20,28,99,86,119,213,237,164</td>\n",
       "      <td>1,1,1,1,1,1,1,1,1</td>\n",
       "      <td>1582992009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20:1580644762,13:1580644765,1:1580644770,127:1...</td>\n",
       "      <td>64054,26565,93755,88510,6344,7615,54240,21927,...</td>\n",
       "      <td>1,4,26,112,86,117,191,234,172</td>\n",
       "      <td>1,1,1,1,1,1,1,1,0</td>\n",
       "      <td>1582992010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39:1580772975,6:1580772981,1:1581178309,2:1581...</td>\n",
       "      <td>64054,64086,63021,88510,93500,7615,54240,21927...</td>\n",
       "      <td>22,4,28,48,105,42,193,236,159</td>\n",
       "      <td>1,1,1,0,1,0,0,0,0</td>\n",
       "      <td>1582992014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6:1580657608,9:1580657612,15:1580657615,127:15...</td>\n",
       "      <td>64054,21531,6599,16721,37078,7615,54240,65505,...</td>\n",
       "      <td>5,16,1,74,133,122,235,218,172</td>\n",
       "      <td>1,1,1,1,1,1,1,1,0</td>\n",
       "      <td>1582992014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>37:1580743545,36:1580743554,25:1580743556,37:1...</td>\n",
       "      <td>64054,66036,6599,88510,76066,20543,83978,37294...</td>\n",
       "      <td>6,1,16,85,73,112,239,172,205</td>\n",
       "      <td>1,1,1,1,1,1,1,1,1</td>\n",
       "      <td>1582992017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 user_click_history  \\\n",
       "0        1  30:1580603130,34:1581178937,15:1581178939,28:1...   \n",
       "1        2  20:1580644762,13:1580644765,1:1580644770,127:1...   \n",
       "2        3  39:1580772975,6:1580772981,1:1581178309,2:1581...   \n",
       "3        4  6:1580657608,9:1580657612,15:1580657615,127:15...   \n",
       "4        5  37:1580743545,36:1580743554,25:1580743556,37:1...   \n",
       "\n",
       "                                       user_protrait  \\\n",
       "0  64054,21804,80911,36504,8867,7615,54240,37294,...   \n",
       "1  64054,26565,93755,88510,6344,7615,54240,21927,...   \n",
       "2  64054,64086,63021,88510,93500,7615,54240,21927...   \n",
       "3  64054,21531,6599,16721,37078,7615,54240,65505,...   \n",
       "4  64054,66036,6599,88510,76066,20543,83978,37294...   \n",
       "\n",
       "                   exposed_items             labels        time  \n",
       "0  1,20,28,99,86,119,213,237,164  1,1,1,1,1,1,1,1,1  1582992009  \n",
       "1  1,4,26,112,86,117,191,234,172  1,1,1,1,1,1,1,1,0  1582992010  \n",
       "2  22,4,28,48,105,42,193,236,159  1,1,1,0,1,0,0,0,0  1582992014  \n",
       "3  5,16,1,74,133,122,235,218,172  1,1,1,1,1,1,1,1,0  1582992014  \n",
       "4   6,1,16,85,73,112,239,172,205  1,1,1,1,1,1,1,1,1  1582992017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = pd.read_csv('./bigdata2021-rl-recsys/trainset.csv', sep = ' ')\n",
    "trainset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['history_num'] = trainset[\"user_click_history\"].apply(lambda row: list(map(lambda t: int(t.split(\":\")[0]), row.split(\",\"))))\n",
    "trainset['state'] = trainset['history_num'].apply(lambda row: row[-10:] if len(row) >= 10 else [0]*(10 - len(row)) + row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array(\n",
    "    list(trainset['state'].apply(sliding_win).values),\n",
    "    dtype = 'int64').reshape(-1, 1)\n",
    "data_y = np.array(\n",
    "    list(trainset['state'].apply(sliding_win_lab).values),\n",
    "    ).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = torch.from_numpy(data_y)\n",
    "data_x = torch.from_numpy(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "one_hot_x = F.one_hot(data_x, 382).reshape(-1, 5, 382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_x = torch.zeros(data_x.size()[0], 382).scatter_(1, data_x, 1).reshape(-1, 5, 382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(one_hot_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(): #single gpu\n",
    "    i = 0\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, item_size, hidden_size, hidden_size2):\n",
    "        super().__init__()\n",
    "        device = try_gpu()\n",
    "        self.item_size = item_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.look_up_table = nn.Parameter(nn.init.xavier_uniform_(torch.normal(0, 1, size = (item_size, hidden_size), requires_grad = True, device = device)))\n",
    "        self.W1 = nn.Parameter(\n",
    "                        nn.init.xavier_uniform_(torch.rand(hidden_size, hidden_size2, device = device))\n",
    "                        )\n",
    "        self.b1 = nn.Parameter(\n",
    "                        nn.init.xavier_uniform_(torch.rand(1, hidden_size2, device = device))\n",
    "                        )\n",
    "        \n",
    "        self.W2 = nn.Parameter(\n",
    "                        nn.init.xavier_uniform_(torch.rand(hidden_size2, item_size, device = device))\n",
    "                        )\n",
    "        self.b2 = nn.Parameter(\n",
    "                        nn.init.xavier_uniform_(torch.rand(1, item_size, device = device))\n",
    "                        )\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.to(device)\n",
    "                                                    \n",
    "    def forward(self, x):  #x.shape:(batch_sz, seq_size: 3, item_size: 382)\n",
    "        batch_sz, seq_sz, _ = x.size()\n",
    "#         H = torch.zeros((batch_sz, self.hidden_size), device = x.device)\n",
    "#         for seq in range(seq_sz):\n",
    "#             x_t = x[:, seq, :]\n",
    "#             H += (x_t @ self.look_up_table )/seq_sz\n",
    "        H = torch.sum(x @ self.look_up_table, dim = 1)/seq_sz\n",
    "        \n",
    "        H2 = torch.relu(H @ self.W1 + self.b1)\n",
    "        output = F.softmax(H2 @ self.W2 + self.b2, dim = 1)\n",
    "        \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train CBOW\n",
    "cbow = CBOW(382,30,32)\n",
    "# cbow.load_state_dict(torch.load('embedding.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0923, -0.0886, -0.0897,  ...,  0.0115,  0.0408, -0.0709],\n",
      "        [-0.1053, -0.1006,  0.0378,  ..., -0.0471, -0.0823, -0.1121],\n",
      "        [ 0.0135, -0.0961,  0.1148,  ...,  0.0577,  0.0003, -0.0845],\n",
      "        ...,\n",
      "        [-0.0023,  0.1113, -0.0199,  ..., -0.0511, -0.0551,  0.0036],\n",
      "        [ 0.1028, -0.1033, -0.0442,  ...,  0.1107,  0.0250, -0.1060],\n",
      "        [-0.1106,  0.0555, -0.0872,  ...,  0.0559,  0.0230,  0.1132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0754,  0.1803,  0.2097,  0.0213,  0.0817, -0.1185, -0.2455,  0.2131,\n",
      "          0.1742, -0.1824, -0.0188,  0.0499,  0.1093, -0.0781,  0.0436,  0.2918,\n",
      "         -0.2615, -0.0875,  0.2230, -0.3035, -0.2608,  0.1237,  0.1943,  0.2498,\n",
      "         -0.0352,  0.1471, -0.1665, -0.1710,  0.0489,  0.0626, -0.0431,  0.1596],\n",
      "        [-0.1529, -0.1107,  0.0449, -0.2511,  0.1123, -0.2734,  0.1867,  0.1104,\n",
      "          0.0390,  0.1723,  0.1477, -0.1666, -0.1214, -0.3086,  0.0959,  0.1721,\n",
      "         -0.2854, -0.1615,  0.1065, -0.1831,  0.0928, -0.0129,  0.1960,  0.1075,\n",
      "          0.2896,  0.0431, -0.1688, -0.1368,  0.0013, -0.1887,  0.0517,  0.0043],\n",
      "        [ 0.2926, -0.0667,  0.2953,  0.2614, -0.2660,  0.1616, -0.2148,  0.0092,\n",
      "          0.2331,  0.1275,  0.1545,  0.1616, -0.1251,  0.1590,  0.2838, -0.2082,\n",
      "          0.1607,  0.2825, -0.0006, -0.0702, -0.1236, -0.3000,  0.2245,  0.0315,\n",
      "         -0.2965, -0.1234, -0.2463,  0.1919, -0.1102,  0.1103, -0.1585, -0.0094],\n",
      "        [ 0.0733, -0.1439, -0.0654, -0.2404, -0.0773, -0.0613, -0.1755, -0.2630,\n",
      "         -0.2948,  0.2389, -0.1605, -0.2308,  0.1590, -0.2590, -0.2393, -0.1020,\n",
      "          0.0297,  0.2686, -0.0151,  0.3090,  0.2315,  0.0958, -0.1448, -0.1542,\n",
      "         -0.1982,  0.2166,  0.2251, -0.1579, -0.2614,  0.1346, -0.2369,  0.0605],\n",
      "        [ 0.0752, -0.1401,  0.1517,  0.0768,  0.0482,  0.0695, -0.0943, -0.1027,\n",
      "          0.2769, -0.1784, -0.0534,  0.1954, -0.2092, -0.0767,  0.1946,  0.1286,\n",
      "         -0.1201,  0.3038, -0.1618,  0.0237, -0.2197,  0.1596, -0.0309,  0.1688,\n",
      "          0.2616,  0.0116, -0.2505, -0.2617,  0.2691,  0.1539,  0.1685,  0.2442],\n",
      "        [-0.1634, -0.1257, -0.0534,  0.1432, -0.0195,  0.1210,  0.0311,  0.1510,\n",
      "          0.2693, -0.1111, -0.1093,  0.0698,  0.1426, -0.1510, -0.2760,  0.0007,\n",
      "         -0.1863, -0.0420,  0.2331, -0.1058, -0.1920,  0.2902,  0.2018, -0.3089,\n",
      "          0.1764, -0.1685, -0.0516,  0.2232, -0.0843, -0.0715, -0.2683, -0.2694],\n",
      "        [ 0.2924,  0.0929,  0.0800,  0.0190, -0.0572,  0.0091, -0.1024,  0.0464,\n",
      "          0.1582,  0.1424,  0.1205,  0.1384,  0.2571, -0.1841, -0.0994, -0.1702,\n",
      "         -0.2808, -0.2052,  0.2767,  0.0288,  0.0743,  0.0524,  0.1765, -0.2483,\n",
      "         -0.2849, -0.0406,  0.0542, -0.0627, -0.1222,  0.1201, -0.1493, -0.1108],\n",
      "        [-0.0082, -0.1456,  0.0053,  0.2287, -0.0365,  0.2854,  0.2789,  0.2539,\n",
      "         -0.0824, -0.2732,  0.1807, -0.1594, -0.2066, -0.0193, -0.1606,  0.2608,\n",
      "         -0.0814,  0.0827, -0.1137,  0.1652,  0.1200, -0.2314, -0.2837,  0.0049,\n",
      "         -0.2596, -0.0135,  0.0024, -0.0566,  0.0205, -0.2578,  0.2690,  0.1529],\n",
      "        [ 0.1066, -0.1426, -0.1271, -0.0370, -0.2152, -0.1657,  0.2607, -0.0029,\n",
      "         -0.2456,  0.1729, -0.1361,  0.2732,  0.0223,  0.1988,  0.2035,  0.2446,\n",
      "         -0.2578,  0.0039,  0.0314,  0.2355,  0.2973,  0.2195,  0.0674,  0.2419,\n",
      "         -0.0885,  0.0614,  0.1495, -0.0869,  0.2550, -0.1092, -0.0128,  0.1075],\n",
      "        [-0.0270, -0.1352,  0.2418,  0.3102,  0.1149, -0.0267,  0.2931, -0.1632,\n",
      "          0.2876, -0.0901, -0.0831, -0.0375, -0.0509, -0.2594, -0.0245, -0.1007,\n",
      "         -0.2539,  0.0527,  0.1606, -0.1207, -0.0082,  0.0089,  0.1394,  0.2362,\n",
      "         -0.2636, -0.1237,  0.1205,  0.0018,  0.2598,  0.2088,  0.2047, -0.2967],\n",
      "        [ 0.2303, -0.3103, -0.0481,  0.1248, -0.1418,  0.1315, -0.0677,  0.0936,\n",
      "          0.1621,  0.1334, -0.2975,  0.2880, -0.2397, -0.2051, -0.1816, -0.1119,\n",
      "         -0.1603, -0.0047,  0.0797,  0.2040,  0.0818,  0.1600,  0.0004,  0.1679,\n",
      "         -0.1708, -0.0236, -0.2220, -0.1817, -0.2846, -0.0442,  0.1116,  0.0169],\n",
      "        [-0.0036,  0.0824,  0.1125,  0.1741, -0.2928,  0.2592, -0.0146,  0.1358,\n",
      "          0.1021,  0.1685, -0.0982,  0.2743,  0.1223, -0.2107, -0.1855,  0.0924,\n",
      "         -0.1747,  0.1523, -0.2408, -0.0674,  0.1630,  0.0991, -0.2194, -0.2879,\n",
      "         -0.0726, -0.0822,  0.2681, -0.2076,  0.0939, -0.1575,  0.2402,  0.3110],\n",
      "        [ 0.2618,  0.0392,  0.1493, -0.2077,  0.2118,  0.0708, -0.2026,  0.2176,\n",
      "         -0.1989, -0.2159,  0.0806, -0.1237,  0.0400, -0.2461, -0.1119,  0.2951,\n",
      "         -0.1764, -0.1470,  0.2605,  0.2637,  0.1939, -0.0228, -0.0058,  0.1328,\n",
      "         -0.1672, -0.2970,  0.1851, -0.1632,  0.0942,  0.2590,  0.1742, -0.2594],\n",
      "        [-0.2757,  0.3071,  0.0085,  0.0144,  0.0370, -0.2779,  0.2911, -0.2519,\n",
      "         -0.1500,  0.1670, -0.0246, -0.2990,  0.0646,  0.1613, -0.1353,  0.0152,\n",
      "          0.1010, -0.0029,  0.2165,  0.0035,  0.1315, -0.1427, -0.2699,  0.0333,\n",
      "          0.1767, -0.2567, -0.0397, -0.1162,  0.0108, -0.1879, -0.0338, -0.0483],\n",
      "        [-0.1431,  0.0175, -0.0090,  0.1757, -0.2174, -0.1146, -0.1595, -0.2749,\n",
      "         -0.2385,  0.1341,  0.0194, -0.0606, -0.1699, -0.1864,  0.1750,  0.0788,\n",
      "          0.1531,  0.0394, -0.2633,  0.1864, -0.2669,  0.1599,  0.1932,  0.2619,\n",
      "          0.2775, -0.0418,  0.2537, -0.1161,  0.3063, -0.1684,  0.2499, -0.0225],\n",
      "        [ 0.2163,  0.1211, -0.2346,  0.2685, -0.2770, -0.2891, -0.0291, -0.2899,\n",
      "          0.1033, -0.0302,  0.0865,  0.0130, -0.0593,  0.0056,  0.0773, -0.2070,\n",
      "         -0.1323, -0.0925, -0.2232, -0.2573, -0.2076,  0.2003, -0.2892, -0.2093,\n",
      "          0.2115, -0.1208,  0.2249,  0.2976, -0.1668, -0.0256,  0.1457,  0.2982],\n",
      "        [-0.0805, -0.2778,  0.1672, -0.1107,  0.2467,  0.2585, -0.1224,  0.2582,\n",
      "          0.2495, -0.1861,  0.2818, -0.1567, -0.1614,  0.1332, -0.2097,  0.2620,\n",
      "          0.0135,  0.1179,  0.0529, -0.0366,  0.0617,  0.0007,  0.1559,  0.1092,\n",
      "          0.0262,  0.2708,  0.1125, -0.1581, -0.1792, -0.1977,  0.1610,  0.0262],\n",
      "        [ 0.0822,  0.0926,  0.0792, -0.1282, -0.0723, -0.2172,  0.1637,  0.1669,\n",
      "         -0.1984,  0.0998,  0.1248,  0.2893,  0.0365, -0.0419, -0.1508, -0.0745,\n",
      "         -0.1130, -0.1713,  0.2246, -0.2986,  0.0291, -0.2499,  0.1231,  0.0587,\n",
      "          0.1325, -0.2839, -0.1679,  0.0724,  0.2481, -0.0919,  0.0594,  0.2516],\n",
      "        [-0.2395, -0.2099,  0.0844,  0.1557,  0.2640, -0.2879,  0.0967, -0.2699,\n",
      "         -0.2304, -0.0616,  0.1853, -0.2948, -0.0258,  0.1391, -0.1059, -0.0224,\n",
      "          0.0242, -0.1111, -0.2988, -0.1210,  0.2533,  0.0659,  0.2111, -0.2084,\n",
      "          0.2776,  0.2227, -0.1272,  0.0015,  0.2540,  0.2628,  0.0625, -0.1579],\n",
      "        [ 0.3028,  0.1201,  0.2156, -0.2002, -0.0638, -0.0051,  0.2712, -0.0912,\n",
      "         -0.2353, -0.2848, -0.2858,  0.2550, -0.1009,  0.2249,  0.0071,  0.0330,\n",
      "          0.1040,  0.1748, -0.1874,  0.0294, -0.0206, -0.1879,  0.2058,  0.2695,\n",
      "         -0.1112, -0.2594, -0.1905, -0.1786, -0.1525, -0.1106, -0.1643,  0.2309],\n",
      "        [ 0.0188,  0.2045, -0.0980,  0.1181,  0.2358,  0.0690, -0.0645, -0.1299,\n",
      "          0.2337,  0.0829,  0.2916,  0.0435, -0.1256,  0.2688,  0.1856,  0.2317,\n",
      "          0.0911,  0.0366, -0.2929,  0.0639, -0.1780, -0.0189,  0.2894,  0.2813,\n",
      "          0.0623,  0.1766,  0.2622, -0.2232, -0.0054,  0.1381, -0.1728,  0.1931],\n",
      "        [ 0.1865,  0.2929, -0.2255,  0.2940, -0.0064,  0.1967,  0.1758, -0.1276,\n",
      "          0.0149,  0.0793,  0.1750,  0.1768,  0.2398,  0.2321, -0.2940,  0.2524,\n",
      "          0.1263,  0.2824,  0.2180, -0.2509,  0.2715, -0.1652,  0.1763, -0.2679,\n",
      "          0.1222,  0.1454,  0.0275, -0.0296, -0.1514,  0.1939,  0.1771,  0.1483],\n",
      "        [-0.1146, -0.1248, -0.1816,  0.0561, -0.0057,  0.1685, -0.1183, -0.2514,\n",
      "          0.0101, -0.1476, -0.2737,  0.2194,  0.2603, -0.2914,  0.1974,  0.0289,\n",
      "          0.2384, -0.1003, -0.3027,  0.0758, -0.1777,  0.1439, -0.1361,  0.1688,\n",
      "          0.0619,  0.2313,  0.2735,  0.0693, -0.1356,  0.0299, -0.1819,  0.2132],\n",
      "        [-0.1991,  0.1790,  0.2195,  0.2619, -0.2163,  0.1463,  0.1961, -0.0985,\n",
      "         -0.1027,  0.2155, -0.2458, -0.3025, -0.0955, -0.1301,  0.0872,  0.0175,\n",
      "          0.1190,  0.1976, -0.1098,  0.0596,  0.0507,  0.2910,  0.0039,  0.1899,\n",
      "         -0.0297,  0.2255, -0.2140,  0.1353, -0.0824,  0.2633,  0.0054, -0.2718],\n",
      "        [ 0.2830, -0.0701,  0.2377,  0.3069,  0.0897,  0.1968, -0.2133,  0.2507,\n",
      "         -0.2605, -0.0554,  0.0688, -0.2079, -0.1244, -0.0895, -0.2093, -0.1600,\n",
      "          0.1776, -0.0529,  0.2910, -0.0587,  0.0613,  0.2944,  0.1398,  0.2977,\n",
      "          0.2831, -0.2611,  0.0396,  0.1975, -0.2975,  0.2667,  0.2714, -0.0262],\n",
      "        [ 0.0766, -0.2735,  0.1513, -0.2833, -0.0522,  0.1842,  0.1305, -0.1874,\n",
      "         -0.0388, -0.1805,  0.0932,  0.2655,  0.2328,  0.1626, -0.1714, -0.1962,\n",
      "         -0.1874, -0.0992,  0.0007, -0.1401,  0.2385, -0.0966,  0.0847, -0.1954,\n",
      "          0.1892, -0.1647, -0.2790,  0.2462, -0.3007, -0.1891,  0.1515,  0.1865],\n",
      "        [-0.1682,  0.1969, -0.2088,  0.0148,  0.1896,  0.0642,  0.0631, -0.0714,\n",
      "          0.1698,  0.3025,  0.0660,  0.2652, -0.0998, -0.1761, -0.0173, -0.0300,\n",
      "          0.1602, -0.3100, -0.2308,  0.2257,  0.2983,  0.2728, -0.0641, -0.1859,\n",
      "          0.1154, -0.1620, -0.1810,  0.0807,  0.2601, -0.1805,  0.1302,  0.1035],\n",
      "        [-0.1410,  0.1417,  0.0399, -0.1697, -0.2699, -0.2895, -0.2147,  0.3012,\n",
      "          0.1580,  0.2478,  0.1231, -0.1191,  0.0883, -0.2802,  0.1684, -0.0027,\n",
      "         -0.1114, -0.2911, -0.1755,  0.0060, -0.1117, -0.0922,  0.0255, -0.2174,\n",
      "          0.2272, -0.0982,  0.2452,  0.0544, -0.2439,  0.0680,  0.1815,  0.1734],\n",
      "        [-0.2065,  0.1087,  0.0565,  0.2441,  0.1427, -0.2127,  0.0113,  0.1572,\n",
      "         -0.0103,  0.1992,  0.0688,  0.1140,  0.2191,  0.1734,  0.0662,  0.0243,\n",
      "         -0.1271, -0.0806,  0.1840,  0.2929,  0.1647, -0.1335, -0.2158, -0.0518,\n",
      "         -0.0846,  0.2490, -0.1047, -0.1237,  0.1572, -0.2274,  0.2473,  0.0131],\n",
      "        [ 0.2354,  0.2169, -0.1360,  0.2414,  0.0913, -0.2544,  0.1746, -0.1460,\n",
      "         -0.2693, -0.2396,  0.1907,  0.1384,  0.0166, -0.0989,  0.2999,  0.3009,\n",
      "          0.0081,  0.2021,  0.1550, -0.0565, -0.1081, -0.2260, -0.1003,  0.0853,\n",
      "          0.0636,  0.2475,  0.1911,  0.0063,  0.2905,  0.2411,  0.1443, -0.0877]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0905, -0.1326, -0.0248,  0.0304, -0.2656,  0.0837, -0.3910,  0.0458,\n",
      "          0.2661, -0.2123, -0.0507,  0.2580,  0.3039,  0.2115, -0.1653, -0.3626,\n",
      "         -0.1325, -0.2680,  0.1901, -0.2235,  0.2075, -0.2780,  0.2657, -0.2366,\n",
      "         -0.1649, -0.3893, -0.1294,  0.2798,  0.0371,  0.2349,  0.0074,  0.1574]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0280, -0.1196, -0.0347,  ..., -0.1038, -0.0210, -0.0805],\n",
      "        [-0.0474,  0.1131,  0.0813,  ...,  0.0250, -0.1193, -0.1017],\n",
      "        [-0.0733,  0.0824, -0.0781,  ...,  0.0691, -0.1070,  0.0659],\n",
      "        ...,\n",
      "        [-0.0396,  0.0795, -0.0910,  ..., -0.0242, -0.1170, -0.0885],\n",
      "        [-0.0131,  0.1146, -0.0720,  ...,  0.0465, -0.1125, -0.0267],\n",
      "        [ 0.0565,  0.1076, -0.0468,  ...,  0.0606, -0.0721, -0.0539]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0309,  0.0695, -0.0282,  0.0365,  0.0761,  0.1119,  0.0256, -0.0811,\n",
      "         -0.0792, -0.0985,  0.0997, -0.0576,  0.0442,  0.1232,  0.1154,  0.1220,\n",
      "         -0.0401, -0.0010,  0.0097,  0.0398,  0.0375, -0.0023,  0.0127,  0.0079,\n",
      "         -0.0875,  0.1115, -0.0406, -0.1150,  0.0291, -0.0381, -0.0639, -0.0623,\n",
      "          0.0780, -0.0849,  0.0972, -0.0149,  0.0109, -0.0322,  0.0101,  0.1239,\n",
      "          0.0236, -0.0343, -0.0665, -0.1024,  0.1157,  0.0217,  0.0879, -0.0589,\n",
      "          0.0263, -0.0659, -0.0705,  0.0815, -0.0867,  0.1205, -0.0296,  0.0828,\n",
      "         -0.0417,  0.0300, -0.1155,  0.0845, -0.0553,  0.0079, -0.0743, -0.1198,\n",
      "          0.1159, -0.0871, -0.0894,  0.1155,  0.0903,  0.0700, -0.0039,  0.0032,\n",
      "         -0.0949, -0.0586,  0.0120, -0.1048,  0.0528, -0.0051, -0.0314,  0.0673,\n",
      "         -0.0495,  0.0410,  0.0733, -0.0938, -0.0517,  0.0671,  0.0468, -0.1208,\n",
      "          0.0596, -0.1065, -0.0447, -0.0206,  0.0866, -0.0153,  0.0704,  0.0264,\n",
      "         -0.0201, -0.0409, -0.1019,  0.0348, -0.0114, -0.0506, -0.1171, -0.0920,\n",
      "         -0.0845,  0.0667, -0.1196,  0.1036, -0.0569, -0.0678,  0.0259, -0.0166,\n",
      "         -0.0021,  0.0637,  0.0620, -0.1182, -0.0099, -0.0860, -0.0495,  0.0690,\n",
      "         -0.0013, -0.0451,  0.0102, -0.0710, -0.0671,  0.1224, -0.0033,  0.1002,\n",
      "          0.0635,  0.0495,  0.0621, -0.0570,  0.0021, -0.1067, -0.1216,  0.0037,\n",
      "          0.1195,  0.0108,  0.0380, -0.0694, -0.0181,  0.0900,  0.1036,  0.1229,\n",
      "          0.0041, -0.0742,  0.0992, -0.0459,  0.0161, -0.0560, -0.0551,  0.0912,\n",
      "          0.0622,  0.0573, -0.1045,  0.0158, -0.1156, -0.0409,  0.1062,  0.1224,\n",
      "         -0.0819,  0.0520,  0.0130, -0.1251,  0.0781, -0.0113, -0.1008, -0.1130,\n",
      "          0.0456, -0.0836, -0.0623,  0.0082, -0.0504,  0.0296, -0.0746, -0.0624,\n",
      "         -0.0155,  0.0227,  0.0053, -0.0378, -0.1056, -0.0874,  0.0861,  0.0805,\n",
      "          0.0609, -0.0533, -0.1161,  0.0169, -0.0200, -0.0292, -0.0517,  0.0195,\n",
      "         -0.1077,  0.0223, -0.1170,  0.0209, -0.0986,  0.0849,  0.0866, -0.1056,\n",
      "         -0.0839, -0.0838, -0.1155, -0.1125, -0.0819, -0.0244,  0.0473,  0.0545,\n",
      "         -0.0802,  0.0099, -0.0906, -0.1236, -0.0850, -0.0034,  0.0472,  0.0921,\n",
      "          0.0476,  0.0245,  0.0284,  0.0791, -0.0569, -0.0670, -0.0986, -0.1175,\n",
      "          0.0431, -0.0859, -0.0100, -0.0623, -0.0600,  0.1144,  0.1188, -0.1023,\n",
      "          0.1056,  0.0151,  0.0626,  0.0948, -0.0646,  0.0346, -0.0040,  0.1159,\n",
      "          0.0146, -0.1182,  0.0198, -0.0704, -0.0991, -0.1239, -0.0391, -0.1046,\n",
      "         -0.0760,  0.0566,  0.0210, -0.0495, -0.0601,  0.0024,  0.0171,  0.1003,\n",
      "         -0.0995, -0.0262, -0.0740,  0.0143,  0.0232, -0.0076, -0.1075,  0.0215,\n",
      "         -0.0222, -0.0313,  0.0913, -0.0694,  0.1087, -0.0601, -0.0902,  0.0438,\n",
      "          0.1187, -0.0427,  0.0147,  0.0211,  0.1196,  0.0685, -0.0265,  0.0322,\n",
      "          0.1021,  0.0421,  0.0495, -0.0561,  0.1015, -0.1024,  0.0546, -0.1035,\n",
      "          0.0945,  0.1208, -0.0512,  0.1202, -0.0895,  0.0048,  0.0413, -0.0796,\n",
      "         -0.0541,  0.0657,  0.0331, -0.0692,  0.0259, -0.0157, -0.0737, -0.0066,\n",
      "         -0.1070,  0.1169, -0.1194,  0.0349, -0.0722,  0.0376,  0.0027, -0.0369,\n",
      "          0.0362,  0.0115, -0.1062,  0.0126, -0.0344,  0.1239, -0.1072,  0.0535,\n",
      "          0.0071, -0.0061, -0.0027,  0.1229, -0.0335, -0.0902, -0.0795, -0.0905,\n",
      "          0.1033, -0.0692, -0.1076,  0.1231, -0.0661,  0.0550, -0.0551, -0.0692,\n",
      "         -0.0598,  0.0103, -0.0862, -0.0686,  0.0354, -0.1174, -0.1124,  0.0075,\n",
      "          0.0372,  0.1057,  0.0749, -0.0363,  0.1237,  0.0638, -0.1177,  0.0059,\n",
      "         -0.0568,  0.0945,  0.0465, -0.0647, -0.0927, -0.0335, -0.1046,  0.0848,\n",
      "         -0.0984,  0.0642, -0.0700, -0.1067,  0.0732,  0.0506, -0.0694, -0.0669,\n",
      "         -0.0505, -0.0124,  0.0189, -0.0941, -0.0112, -0.0649, -0.0665,  0.0311,\n",
      "         -0.0848, -0.0691, -0.0189, -0.0537,  0.1118, -0.1035]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameters in cbow.parameters():\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:4.543539047241211\n",
      "loss:4.201603412628174\n",
      "loss:4.3856401443481445\n",
      "loss:4.20010232925415\n",
      "loss:4.246908664703369\n",
      "loss:4.072366714477539\n",
      "loss:4.19942045211792\n",
      "loss:4.0240631103515625\n",
      "loss:4.0638957023620605\n",
      "loss:3.9501426219940186\n",
      "loss:4.066921710968018\n",
      "loss:3.83845591545105\n",
      "loss:3.8177998065948486\n",
      "loss:3.892683982849121\n",
      "loss:3.8095293045043945\n",
      "loss:3.8375251293182373\n",
      "loss:3.9538707733154297\n",
      "loss:3.9706623554229736\n",
      "loss:3.7868030071258545\n",
      "loss:3.918647050857544\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss(reduction='mean')\n",
    "# optimizer = torch.optim.Adam(cbow.parameters(), lr = 0.01)\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr = 0.01)\n",
    "device = try_gpu()\n",
    "for epoch in range(20):\n",
    "    for X, Y in train_loader:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = cbow(X)\n",
    "# #         print(y_hat.device)\n",
    "        loss = loss_fn(torch.log(y_hat), Y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'loss:{loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor(240, device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "device = try_gpu()\n",
    "for X, Y in train_loader:\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    y_hat = cbow(X)\n",
    "    print(X[0,:,:])\n",
    "    print(Y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0706, device='cuda:0', grad_fn=<UnbindBackward>),\n",
       " tensor(51, device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_hat[0]), torch.argmax(y_hat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor(196, device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0884, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(196, device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_hat = cbow(X)\n",
    "i = 23\n",
    "print(X[i,:,:])\n",
    "print(Y[i])\n",
    "torch.max(y_hat[i]), torch.argmax(y_hat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow.state_dict(),'embedding.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vec = cbow.look_up_table.data.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [str(item) for item in range(382)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = dict(zip(keys, item_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embed_dict.npy', embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict=np.load('embed_dict.npy',allow_pickle=True).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
